
===== blink_detector.py =====

"""Eye detection and blink analysis module."""

import cv2
import numpy as np
import time
import logging
import dlib
from typing import Tuple, Optional
from collections import deque

from config import Config

class BlinkDetector:
    """Handles eye detection and blink analysis using facial landmarks."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load eye detectors (as fallback if no dlib)
        self.eye_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        if self.eye_detector.empty():
            self.logger.warning("Failed to load eye detector cascade")
        
        # Load dlib face detector + shape predictor
        self.dlib_detector = dlib.get_frontal_face_detector()
        try:
            self.dlib_predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
            self.using_dlib = True
            self.logger.info("Using dlib for facial landmark detection")
        except Exception as e:
            self.logger.warning(f"Could not load dlib shape predictor: {e}")
            self.logger.warning("Falling back to Haar cascade for eye detection")
            self.using_dlib = False
        
        # Blink detection variables
        self.blink_threshold = config.BLINK_THRESHOLD
        self.min_blink_frames = config.MIN_BLINK_FRAMES
        self.blink_frames = 0
        self.blink_counter = 0
        self.blink_detected = False
        self.last_blink_time = time.time()
        
        # Track EAR
        self.ear_history = deque(maxlen=30)
        self.eye_state = "open"  # can be "open", "closing", "closed", "opening"
        self.eye_state_start = time.time()

        # [CHANGED] Rate-limit debug logs to once per second
        self.last_debug_time = 0.0
    
    def calculate_ear(self, eye_points: np.ndarray) -> float:
        # Eye Aspect Ratio
        A = np.linalg.norm(eye_points[1] - eye_points[5])
        B = np.linalg.norm(eye_points[2] - eye_points[4])
        C = np.linalg.norm(eye_points[0] - eye_points[3])
        if C == 0:
            return 0
        return (A + B) / (2.0 * C)
    
    def detect_blinks_dlib(self, frame: np.ndarray,
                           face_rect: Tuple[int,int,int,int]) -> bool:
        """Detect blinks using dlib EAR. Draw lines on `frame` for debug."""
        if face_rect is None:
            return False
        
        x, y, w, h = face_rect
        rect = dlib.rectangle(x, y, x + w, y + h)
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        landmarks = self.dlib_predictor(gray, rect)
        
        left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])
        right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])
        
        left_ear = self.calculate_ear(left_eye)
        right_ear = self.calculate_ear(right_eye)
        avg_ear = (left_ear + right_ear) / 2.0
        self.ear_history.append(avg_ear)
        
        # Draw eye contours for debugging, rate-limited
        now = time.time()
        if now - self.last_debug_time > 1.0:
            for eye in [left_eye, right_eye]:
                for i in range(len(eye)):
                    pt1 = tuple(eye[i])
                    pt2 = tuple(eye[(i+1) % 6])
                    cv2.line(frame, pt1, pt2, (0,255,0), 1)
        
        # [CHANGED] Rate-limit debug logs
        if now - self.last_debug_time > 1.0:
            self.logger.debug(f"EAR: {avg_ear:.2f} (Threshold: {self.blink_threshold:.2f})")
        
        blink_detected_now = False
        
        if avg_ear < self.blink_threshold:
            self.blink_frames += 1
            if self.eye_state == "open":
                self.eye_state = "closing"
                self.eye_state_start = now
            elif (self.eye_state == "closing"
                  and (now - self.eye_state_start) > 0.1):
                self.eye_state = "closed"
                self.eye_state_start = now
        else:
            if (self.eye_state == "closed"
                and self.blink_frames >= self.min_blink_frames
                and (now - self.last_blink_time) > self.config.MIN_BLINK_INTERVAL):
                self.blink_counter += 1
                self.blink_detected = True
                blink_detected_now = True
                
                # [CHANGED] Rate-limit "BLINK DETECTED" info
                if now - self.last_debug_time > 1.0:
                    self.logger.info(f"BLINK DETECTED! Counter: {self.blink_counter}")
                
                self.last_blink_time = now
            
            self.eye_state = "open" if self.eye_state != "closed" else "opening"
            self.eye_state_start = now
            self.blink_frames = 0
        
        # Display EAR + blink count in face ROI
        face_roi = frame[y : y + h, x : x + w]
        cv2.putText(face_roi, f"EAR: {avg_ear:.2f}", (10,40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        cv2.putText(face_roi, f"Blinks: {self.blink_counter}", (10,20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        
        # Update debug timestamp if needed
        if now - self.last_debug_time > 1.0:
            self.last_debug_time = now
        
        return blink_detected_now
    
    def detect_blinks_haar(self, face_roi: np.ndarray,
                           frame: np.ndarray,
                           face_rect: Tuple[int,int,int,int]) -> bool:
        """Fallback blink detection with Haar. Extremely simplistic."""
        if face_roi.shape[0]<20 or face_roi.shape[1]<20:
            return False
        
        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        gray_face = cv2.equalizeHist(gray_face)
        
        eyes = self.eye_detector.detectMultiScale(gray_face, 1.1,3, minSize=(20,20))
        if len(eyes)<2:
            eyes = self.eye_detector.detectMultiScale(gray_face, 1.05,2, minSize=(15,15))
        
        x,y,w,h = face_rect
        blink_detected_now = False
        now = time.time()
        
        if len(eyes)==0:
            if (now - self.last_blink_time) > self.config.MIN_BLINK_INTERVAL:
                self.blink_counter += 1
                self.blink_detected = True
                blink_detected_now = True
                self.logger.debug(f"BLINK DETECTED! Counter: {self.blink_counter}")
                self.last_blink_time = now
        else:
            # draw eyes for debug
            if now - self.last_debug_time > 1.0:
                for (ex,ey,ew,eh) in eyes:
                    cv2.rectangle(frame, (x+ex,y+ey), (x+ex+ew, y+ey+eh), (0,255,0),1)
        
        # show blink count
        roi = frame[y : y+h, x : x+w]
        cv2.putText(roi, f"Blinks: {self.blink_counter}", (10,20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        
        if now - self.last_debug_time > 1.0:
            self.last_debug_time = now
        
        return blink_detected_now
    
    def detect_blinks(self,
                      frame: np.ndarray,
                      face_rect: Tuple[int,int,int,int],
                      face_roi: np.ndarray) -> bool:
        """
        Decide which method to use: dlib or Haar.
        We draw debug lines right on `frame`.
        """
        if self.using_dlib:
            return self.detect_blinks_dlib(frame, face_rect)
        else:
            return self.detect_blinks_haar(face_roi, frame, face_rect)
    
    def reset(self) -> None:
        self.blink_counter = 0
        self.blink_detected = False
        self.blink_frames = 0
        self.eye_state = "open"
        self.last_blink_time = time.time()
        self.ear_history.clear()
        self.last_debug_time = 0.0



===== challenge_manager.py =====

"""Challenge management module for liveness verification."""

import time
import random
import logging
from typing import Optional, Tuple

from config import Config

class ChallengeManager:
    """Manages challenge issuance and verification."""
    
    def __init__(self, config: Config, speech_recognizer=None, blink_detector=None):
        self.config = config
        self.speech_recognizer = speech_recognizer
        self.blink_detector = blink_detector
        self.logger = logging.getLogger(__name__)
        
        self.current_challenge = None
        self.challenge_completed = False
        self.challenge_start_time = None
        self.challenge_timeout = config.CHALLENGE_TIMEOUT
        self.available_challenges = config.CHALLENGES
        self.challenge_action_completed = False
        self.challenge_word_completed = False
        self.verification_result = None
        self.action_completion_time = None
        self.word_completion_time = None
    
    def issue_new_challenge(self) -> str:
        self.current_challenge = random.choice(self.available_challenges)
        self.challenge_start_time = time.time()
        self.challenge_completed = False
        self.challenge_action_completed = False
        self.challenge_word_completed = False
        self.verification_result = None
        self.action_completion_time = None
        self.word_completion_time = None
        
        if self.speech_recognizer:
            self.speech_recognizer.reset()
        if self.blink_detector:
            self.blink_detector.reset()
            self.logger.debug("Blink counter reset for new challenge")
        
        self.logger.info(f"New challenge issued: {self.current_challenge}")
        return self.current_challenge
    
    def verify_challenge(self, head_pose: str, blink_counter: int, last_speech: str) -> bool:
        if self.current_challenge is None:
            return False
        
        self.logger.debug(f"Verifying - Head: {head_pose}, Blinks: {blink_counter}, Speech: '{last_speech}'")
        
        elapsed = time.time() - self.challenge_start_time
        if elapsed > self.challenge_timeout:
            self.verification_result = "FAIL"
            self.current_challenge = None
            if self.speech_recognizer:
                self.speech_recognizer.reset()
            self.logger.info("Challenge timed out")
            return True
        
        c = self.current_challenge.lower()
        
        # Action check
        if not self.challenge_action_completed:
            if "turn left" in c and head_pose == "left":
                self.challenge_action_completed = True
                self.action_completion_time = time.time()
                self.logger.debug("LEFT ACTION COMPLETED!")
            elif "turn right" in c and head_pose == "right":
                self.challenge_action_completed = True
                self.action_completion_time = time.time()
                self.logger.debug("RIGHT ACTION COMPLETED!")
            elif "look up" in c and head_pose == "up":
                self.challenge_action_completed = True
                self.action_completion_time = time.time()
                self.logger.debug("UP ACTION COMPLETED!")
            elif "look down" in c and head_pose == "down":
                self.challenge_action_completed = True
                self.action_completion_time = time.time()
                self.logger.debug("DOWN ACTION COMPLETED!")
            elif "blink twice" in c and blink_counter >= 2:
                self.challenge_action_completed = True
                self.action_completion_time = time.time()
                self.logger.debug(f"BLINK ACTION COMPLETED! Counter: {blink_counter}")
            elif "nod your head" in c:
                # if you detect nod
                pass
            elif "shake your head" in c:
                # if you detect shake
                pass
        
        # Word check
        if not self.challenge_word_completed:
            if "say blue" in c and "blue" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("BLUE WORD COMPLETED!")
            elif "say red" in c and "red" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("RED WORD COMPLETED!")
            elif "say sky" in c and "sky" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("SKY WORD COMPLETED!")
            elif "say ground" in c and "ground" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("GROUND WORD COMPLETED!")
            elif "say hello" in c and "hello" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("HELLO WORD COMPLETED!")
        
        # Check concurrency
        if self.challenge_action_completed and self.challenge_word_completed:
            diff = abs((self.action_completion_time or 0) - (self.word_completion_time or 0))
            self.logger.debug(
                f"Time diff between action & speech: {diff:.2f}s "
                f"(max {self.config.ACTION_SPEECH_WINDOW:.2f}s)"
            )
            if diff <= self.config.ACTION_SPEECH_WINDOW:
                self.challenge_completed = True
                self.verification_result = "PASS"
                self.current_challenge = None
                if self.speech_recognizer:
                    self.speech_recognizer.reset()
                self.logger.info("Challenge PASSED!")
                return True
            else:
                self.logger.debug(f"Action & speech not concurrent (diff: {diff:.2f}s)")
        
        return False
    
    def get_challenge_status(self) -> Tuple[Optional[str],bool,bool,Optional[str]]:
        return (
            self.current_challenge,
            self.challenge_action_completed,
            self.challenge_word_completed,
            self.verification_result
        )
    
    def get_challenge_time_remaining(self) -> float:
        if self.current_challenge is None or self.challenge_start_time is None:
            return 0
        elapsed = time.time() - self.challenge_start_time
        return max(0, self.challenge_timeout - elapsed)
    
    def update(self, head_pose: str, blink_counter: int, last_speech: str) -> None:
        """
        Update the challenge manager with the latest detection results.
        
        Args:
            blink_counter: Number of blinks detected
            head_pose: Current head pose ("left", "right", "up", "down", etc.)
            last_speech: Last recognized speech
        """
        if self.current_challenge:
            self.verify_challenge(head_pose, blink_counter, last_speech)

    # [CHANGED] Added a reset method so we can call challenge_manager.reset()
    def reset(self) -> None:
        self.current_challenge = None
        self.challenge_completed = False
        self.challenge_start_time = None
        self.challenge_action_completed = False
        self.challenge_word_completed = False
        self.verification_result = None
        self.action_completion_time = None
        self.word_completion_time = None



===== config.py =====

"""Configuration settings for the liveness detection system."""

import os

class Config:
    # Debug mode
    DEBUG = True
    
    # Show debug frame with eye tracking polygons, EAR values, etc.
    SHOW_DEBUG_FRAME = True

    # Session timeout in seconds
    SESSION_TIMEOUT = 120
    
    # Camera settings
    CAMERA_WIDTH = 640
    CAMERA_HEIGHT = 480
    
    # Face detection parameters
    FACE_CONFIDENCE_THRESHOLD = 0.9
    FACE_NMS_THRESHOLD = 0.3
    
    # Head pose thresholds (normalized)
    HEAD_POSE_THRESHOLD_X = 0.06  # 8% of half frame width
    HEAD_POSE_THRESHOLD_Y_UP = 0.08  # 8% for looking up
    HEAD_POSE_THRESHOLD_Y_DOWN = 0.10  # 10% for looking down
    
    # History lengths for tracking
    LANDMARK_HISTORY_MAX = 30  # Maximum frames to keep in landmark history
    FACE_POSITION_HISTORY_LENGTH = 30  # Length of face position/angle history
    
    # Blink detection parameters
    BLINK_THRESHOLD = 0.25  # EAR threshold for blink detection
    MIN_BLINK_FRAMES = 1    # Minimum consecutive frames below threshold to count as blink
    MIN_BLINK_INTERVAL = 0.1  # Minimum time between blinks (seconds)
    
    # Challenge parameters
    CHALLENGE_TIMEOUT = 10  # seconds
    ACTION_SPEECH_WINDOW = 5.0  # seconds allowed between action and speech
    
    # Speech recognition parameters
    SPEECH_TIMEOUT = 5  # seconds
    SPEECH_PHRASE_LIMIT = 2  # seconds
    SPEECH_SAMPLING_RATE = 16000
    SPEECH_BUFFER_SIZE = 1024
    SPEECH_KEYWORDS = [
        "blue /1e-3/",
        "red /1e-3/",
        "sky /1e-3/",
        "ground /1e-3/",
        "hello /1e-3/",
        "verify /1e-3/",
        "noise /1e-1/",
    ]
    
    # Liveness scoring
    MIN_CONSECUTIVE_LIVE_FRAMES = 5
    MIN_CONSECUTIVE_FAKE_FRAMES = 5
    
    # Logging
    LOGGING_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Available challenges
    CHALLENGES = [
        "Turn left and say blue", 
        "Turn right and say red", 
        "Look up and say sky", 
        "Look down and say ground", 
        "Blink twice and say hello"
    ]

    # [CHANGED] SSL and host/port in config
    CERTFILE = 'cert.pem'
    KEYFILE = 'key.pem'
    HOST = '0.0.0.0'
    PORT = int(os.environ.get('PORT', 8080))
    BASE_URL = 'https://192.168.8.126:8080'  # Configurable base URL for QR code



===== face_detector.py =====

"""Face detection and head pose estimation module."""

import cv2
import numpy as np
from collections import deque
import logging
from typing import Tuple, Optional

from config import Config

class FaceDetector:
    """Handles face detection and head pose estimation."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load cascade
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.logger.info(f"Attempting to load cascade from: {cascade_path}")
        self.face_detector = cv2.CascadeClassifier(cascade_path)
        if self.face_detector.empty():
            self.logger.error("Failed to load face detector cascade")
            raise ValueError("Failed to load face detector cascade")
        self.logger.info("Face detector cascade loaded successfully")
        
        self.face_positions = deque(maxlen=30)
        self.face_angles = deque(maxlen=30)
        self.head_pose = "center"
        self.movement_detected = False
        
        # [CHANGED] Rate-limit debug logs
        self.last_debug_time = 0.0
    
    def detect_face(self, frame: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[Tuple[int,int,int,int]]]:
        now = cv2.getTickCount() / cv2.getTickFrequency()
        
        if frame is None or frame.size == 0:
            self.logger.error("Received empty or None frame")
            return None, None
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_detector.detectMultiScale(gray, 1.1, 5)
        
        if len(faces) == 0:
            faces = self.face_detector.detectMultiScale(gray, 1.05, 3, minSize=(30, 30))
        if len(faces) == 0:
            faces = self.face_detector.detectMultiScale(gray, 1.03, 2, minSize=(20, 20))
        
        # Fallback logic: If no face is detected, use the last known position
        if len(faces) == 0 and len(self.face_positions) > 0:
            last_x, last_y = self.face_positions[-1]
            est_size = 150  # Estimated size for fallback
            x = int(last_x - est_size // 2)
            y = int(last_y - est_size // 2)
            w = est_size
            h = est_size
            
            if 0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]:
                w = min(w, frame.shape[1] - x)
                h = min(h, frame.shape[0] - y)
                if w > 0 and h > 0:
                    face_roi = frame[y:y+h, x:x+w]
                    if now - self.last_debug_time > 1.0:
                        self.logger.debug("Using estimated face position fallback")
                        self.last_debug_time = now
                    return face_roi, (x, y, w, h)
            
            return None, None
        
        if len(faces) == 0:
            if now - self.last_debug_time > 1.0:
                self.logger.debug("No face detected in frame")
                self.last_debug_time = now
            return None, None
        
        face_rect = max(faces, key=lambda rect: rect[2] * rect[3])
        x, y, w, h = face_rect
        x = max(0, x)
        y = max(0, y)
        w = min(w, frame.shape[1] - x)
        h = min(h, frame.shape[0] - y)
        if w <= 0 or h <= 0:
            if now - self.last_debug_time > 1.0:
                self.logger.debug("Detected face ROI invalid")
                self.last_debug_time = now
            return None, None
        
        face_roi = frame[y:y+h, x:x+w]
        
        if now - self.last_debug_time > 1.0:
            self.logger.debug(f"Face detected at: ({x}, {y}, {w}, {h})")
            self.last_debug_time = now
        
        return face_roi, (x, y, w, h)
    
    def detect_movement(self, face_rect: Tuple[int,int,int,int]) -> bool:
        if face_rect is None:
            return False
        x,y,w,h = face_rect
        cx = x + w/2
        cy = y + h/2
        
        self.face_positions.append((cx,cy))
        if len(self.face_positions)<2:
            return False
        
        positions = list(self.face_positions)
        movement=0
        for i in range(1,len(positions)):
            dx = positions[i][0]-positions[i-1][0]
            dy = positions[i][1]-positions[i-1][1]
            movement += np.sqrt(dx*dx + dy*dy)
        
        avg_movement = movement / (len(positions)-1)
        self.movement_detected = avg_movement>2.0
        return self.movement_detected
    
    def detect_head_pose(self, frame: np.ndarray,
                         face_rect: Tuple[int,int,int,int]) -> str:
        if face_rect is None:
            return self.head_pose
        
        x,y,w,h = face_rect
        face_cx = x + w/2
        face_cy = y + h/2
        frame_cx = frame.shape[1]/2
        frame_cy = frame.shape[0]/2
        
        x_offset = face_cx - frame_cx
        y_offset = face_cy - frame_cy
        
        x_offset_norm = x_offset/(frame.shape[1]/2)
        y_offset_norm = y_offset/(frame.shape[0]/2)
        
        self.face_angles.append((x_offset_norm,y_offset_norm))
        
        if len(self.face_angles)>=5:
            angles_list = list(self.face_angles)
            avg_x = sum(a[0] for a in angles_list)/len(angles_list)
            avg_y = sum(a[1] for a in angles_list)/len(angles_list)
            
            x_thr = self.config.HEAD_POSE_THRESHOLD_X
            y_thr_up = self.config.HEAD_POSE_THRESHOLD_Y_UP
            y_thr_down = self.config.HEAD_POSE_THRESHOLD_Y_DOWN
            
            old_pose = self.head_pose
            if avg_x < -x_thr:
                self.head_pose="right"
            elif avg_x > x_thr:
                self.head_pose="left"
            elif avg_y < -y_thr_up:
                self.head_pose="up"
            elif avg_y > y_thr_down:
                self.head_pose="down"
            else:
                self.head_pose="center"
            
            now = time.time()
            if old_pose != self.head_pose and now - self.last_debug_time > 1.0:
                self.logger.debug(f"{self.head_pose.upper()} detected!")
                self.last_debug_time = now
            
            # draw line for debug
            center_x = int(frame.shape[1]/2)
            center_y = int(frame.shape[0]/2)
            dir_x = int(center_x + avg_x*100)
            dir_y = int(center_y + avg_y*100)
            cv2.line(frame, (center_x,center_y), (dir_x,dir_y), (0,255,255),2)
        
        return self.head_pose
    
    def draw_face_info(self, frame: np.ndarray,
                       face_rect: Tuple[int,int,int,int],
                       status: str,
                       score: float) -> None:
        if face_rect is None:
            return
        x,y,w,h = face_rect
        
        color = (0,0,255)  # default red
        if status=="Live Person":
            color = (0,255,0)
        elif status=="Analyzing...":
            color = (0,165,255)
        
        cv2.rectangle(frame, (x,y), (x+w, y+h), color,2)
        
        cv2.putText(frame, f"Status: {status}", (x,y-40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color,2)
        cv2.putText(frame, f"Score: {score:.2f}", (x,y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color,2)
        
        cv2.putText(frame, f"Head: {self.head_pose}",
                    (10, frame.shape[0]-50),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                    (255,255,255),2)



===== liveness_detector.py =====

"""Main liveness detection module integrating all components."""

import cv2
import numpy as np
import time
import logging
from typing import Tuple, Optional
import dlib

from config import Config
from face_detector import FaceDetector
from blink_detector import BlinkDetector
from speech_recognizer import SpeechRecognizer
from challenge_manager import ChallengeManager
from action_detector import ActionDetector

class LivenessDetector:
    """Main class for liveness detection integrating all components."""
    
    def __init__(self, config: Config):
        """Initialize the liveness detector with configuration."""
        self.config = config
        logging_level = logging.DEBUG if config.DEBUG else logging.INFO
        logging.basicConfig(
            level=logging_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        if not config.DEBUG:
            for handler in logging.root.handlers:
                handler.setLevel(logging.WARNING)
        
        self.face_detector = FaceDetector(config)
        self.blink_detector = BlinkDetector(config)
        self.action_detector = ActionDetector(config)
        self.speech_recognizer = SpeechRecognizer(config)
        
        self.challenge_manager = ChallengeManager(
            config,
            speech_recognizer=self.speech_recognizer,
            blink_detector=self.blink_detector
        )
        
        self.consecutive_live_frames = 0
        self.consecutive_fake_frames = 0
        self.status = "Waiting for verification..."
        self.liveness_score = 0.0
        self.duress_detected = False
        
        self.start_challenge()
        self.logger.debug("LivenessDetector initialized")
    
    def detect_liveness(self, frame: np.ndarray) -> Tuple[np.ndarray, bool]:
        """Process a frame for liveness detection (older method)."""
        display_frame = frame.copy()
        
        face_roi, face_rect = self.face_detector.detect_face(frame)
        
        if face_roi is None:
            cv2.putText(display_frame, "No face detected", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            return display_frame, False
        
        self.face_detector.detect_movement(face_rect)
        head_pose = self.action_detector.detect_head_pose(display_frame, face_rect)
        self.blink_detector.detect_blinks(frame, face_rect, face_roi)
        last_speech = self.speech_recognizer.get_last_speech()
        
        self.challenge_manager.update(head_pose, self.blink_detector.blink_counter, last_speech)
        
        challenge_text, action_completed, word_completed, verification_result = \
            self.challenge_manager.get_challenge_status()
        
        if challenge_text is not None:
            self.challenge_manager.verify_challenge(
                head_pose, self.blink_detector.blink_counter, last_speech
            )
            
            cv2.putText(display_frame, f"Challenge: {challenge_text}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            time_remaining = self.challenge_manager.get_challenge_time_remaining()
            cv2.putText(display_frame, f"Time: {time_remaining:.1f}s", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            action_status = "✓" if action_completed else "✗"
            word_status = "✓" if word_completed else "✗"
            cv2.putText(display_frame, f"Action: {action_status}", (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(display_frame, f"Word: {word_status}", (10, 120),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            if verification_result == "PASS":
                cv2.putText(display_frame, "VERIFICATION PASSED", (50, 200),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                return display_frame, True
            elif verification_result == "FAIL":
                cv2.putText(display_frame, "VERIFICATION FAILED", (50, 200),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                return display_frame, True
        else:
            self.start_challenge()
        
        self.face_detector.draw_face_info(display_frame, face_rect, self.status, self.liveness_score)
        cv2.putText(display_frame, f"Speech: {last_speech}", (10, display_frame.shape[0]-20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return display_frame, False

    def reset(self) -> None:
        """Reset all components to initial state."""
        self.blink_detector.reset()
        self.speech_recognizer.reset()
        self.challenge_manager.reset()
        self.consecutive_live_frames = 0
        self.consecutive_fake_frames = 0
        self.status = "Waiting for verification..."
        self.liveness_score = 0.0
        self.duress_detected = False
        self.start_challenge()
        self.logger.debug("LivenessDetector reset")

    def start_challenge(self):
        """Start a new challenge."""
        self.challenge_manager.issue_new_challenge()
        self.speech_recognizer.start_listening()
        challenge_text, _, _, _ = self.challenge_manager.get_challenge_status()
        if challenge_text:
            target_word = challenge_text.split()[-1]
            self.speech_recognizer.set_target_word(target_word)
        self.logger.debug(f"New challenge started: {challenge_text}")
    
    def process_frame(self, frame):
        """Process a frame for liveness detection (newer approach)."""
        self.logger.debug("Processing frame in LivenessDetector")
        if frame is None or frame.size == 0:
            self.logger.error("Frame is None or empty in process_frame")
            return {
                'display_frame': None,
                'debug_frame': None,
                'verification_result': 'PENDING',
                'exit_flag': False,
                'challenge_text': None,
                'action_completed': False,
                'word_completed': False,
                'time_remaining': 0,
                'duress_detected': False
            }
        
        display_frame = frame.copy()
        debug_frame = frame.copy() if self.config.SHOW_DEBUG_FRAME else None
        
        face_roi, face_rect = self.face_detector.detect_face(display_frame)
        
        head_pose = None
        if face_roi is None:
            self.logger.debug("No face detected")
            cv2.putText(display_frame, "No face detected", (30, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            if debug_frame is not None:
                cv2.putText(debug_frame, "No face detected", (30, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
        else:
            self.logger.debug(f"Face detected at {face_rect}")
            self.blink_detector.detect_blinks(frame, face_rect, face_roi)
            self.logger.debug(f"Blink count: {self.blink_detector.blink_counter}")
            
            if self.config.SHOW_DEBUG_FRAME:
                self.logger.debug("Generating debug frame with landmarks")
                gray_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
                x, y, w, h = face_rect
                dlib_rect = dlib.rectangle(0, 0, w, h)
                landmarks = self.blink_detector.dlib_predictor(gray_roi, dlib_rect)
                
                left_eye = [(landmarks.part(i).x + x, landmarks.part(i).y + y) for i in range(36, 42)]
                right_eye = [(landmarks.part(i).x + x, landmarks.part(i).y + y) for i in range(42, 48)]
                
                cv2.polylines(debug_frame, [np.array(left_eye)], True, (0, 255, 0), 1)
                cv2.polylines(debug_frame, [np.array(right_eye)], True, (0, 255, 0), 1)
                
                left_ear = self.blink_detector.calculate_ear(np.array(left_eye) - np.array([x, y]))
                right_ear = self.blink_detector.calculate_ear(np.array(right_eye) - np.array([x, y]))
                
                left_center = np.mean(np.array(left_eye), axis=0).astype(int)
                right_center = np.mean(np.array(right_eye), axis=0).astype(int)
                cv2.putText(debug_frame, f"L: {left_ear:.2f}", 
                            (left_center[0] - 20, left_center[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                cv2.putText(debug_frame, f"R: {right_ear:.2f}", 
                            (right_center[0] - 20, right_center[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                self.logger.debug(f"Debug frame generated: EAR L={left_ear:.2f}, R={right_ear:.2f}")
        
        head_pose = self.action_detector.detect_head_pose(display_frame, face_rect)
        self.logger.debug(f"Head pose: {head_pose}")
        last_speech = self.speech_recognizer.get_last_speech()
        
        if last_speech.lower() == "verify":
            self.duress_detected = True
            self.logger.info("Duress detected: 'verify' spoken")
        
        self.challenge_manager.update(head_pose, self.blink_detector.blink_counter, last_speech)
        
        challenge_text, action_completed, word_completed, verification_result = \
            self.challenge_manager.get_challenge_status()
        time_left = self.challenge_manager.get_challenge_time_remaining()
        self.logger.debug(f"Challenge status: text={challenge_text}, action={action_completed}, "
                         f"word={word_completed}, result={verification_result}, time={time_left:.1f}s")
        
        if word_completed and challenge_text:
            target_action = challenge_text.split()[1].lower()
            action_completed = (head_pose == target_action)
            self.logger.debug(f"Word spoken, verifying action: expected={target_action}, detected={head_pose}")
            self.challenge_manager.action_completed = action_completed
        
        final_result = 'PENDING'
        exit_flag = False
        
        if verification_result != "PENDING":
            last_speech = self.speech_recognizer.get_last_speech()
            if debug_frame is not None:
                cv2.putText(debug_frame, f"Speech: {last_speech}", (10, 150),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(debug_frame, f"Blinks: {self.blink_detector.blink_counter}", (10, 180),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            if self.duress_detected:
                self.status = "UNDER DURESS DETECTED"
                cv2.putText(debug_frame if debug_frame is not None else display_frame,
                            "DURESS DETECTED", (20, display_frame.shape[0]-20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (239, 234, 79), 2)
                final_result = 'FAIL'
                exit_flag = True
            elif verification_result == "PASS":
                self.status = "VERIFICATION PASSED"
                cv2.putText(debug_frame if debug_frame is not None else display_frame,
                            "VERIFICATION PASSED", (20, display_frame.shape[0]-20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                final_result = 'PASS'
                exit_flag = True
            elif verification_result == "FAIL":
                self.status = "VERIFICATION FAILED"
                cv2.putText(debug_frame if debug_frame is not None else display_frame,
                            "VERIFICATION FAILED", (20, display_frame.shape[0]-20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                final_result = 'FAIL'
                exit_flag = True
            self.logger.debug(f"Verification result: {final_result}")
        else:
            if not challenge_text:
                self.start_challenge()
                challenge_text, action_completed, word_completed, verification_result = \
                    self.challenge_manager.get_challenge_status()
                time_left = self.challenge_manager.get_challenge_time_remaining()
                self.logger.debug("Issued new challenge due to none active")
        
        # Draw face info on both frames
        self.face_detector.draw_face_info(display_frame, face_rect, self.status, self.liveness_score)
        cv2.putText(display_frame, f"Speech: {self.speech_recognizer.get_last_speech()}",
                    (10, display_frame.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        if self.config.SHOW_DEBUG_FRAME and debug_frame is not None:
            self.face_detector.draw_face_info(debug_frame, face_rect, self.status, self.liveness_score)
        
        cv2.putText(display_frame, f"Head Pose: {head_pose if head_pose else 'None'}", (10, 210),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Challenge: {challenge_text}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Action completed: {action_completed}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Word completed: {word_completed}", (10, 90),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Time left: {time_left:.1f}s", (10, 120),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        if debug_frame is not None:
            cv2.putText(debug_frame, f"Head Pose: {head_pose if head_pose else 'None'}", (10, 210),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Challenge: {challenge_text}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Action completed: {action_completed}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Word completed: {word_completed}", (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Time left: {time_left:.1f}s", (10, 120),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        if action_completed and word_completed:
            self.liveness_score = 1.0
        else:
            self.liveness_score = 0.0
        
        self.logger.debug("Frame processing completed")
        return {
            'display_frame': display_frame,
            'debug_frame': debug_frame,
            'verification_result': final_result,
            'exit_flag': exit_flag,
            'challenge_text': challenge_text,
            'action_completed': action_completed,
            'word_completed': word_completed,
            'time_remaining': time_left,
            'duress_detected': self.duress_detected
        }



===== main.py =====

"""Main application for liveness detection (CLI version)."""

import cv2
import time
import argparse
import logging
from typing import Optional

from config import Config
from liveness_detector import LivenessDetector

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Liveness Detection System")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--camera", type=int, default=0, help="Camera index")
    return parser.parse_args()

def main():
    args = parse_args()
    
    config = Config()
    config.DEBUG = args.debug
    
    logging_level = logging.DEBUG if config.DEBUG else logging.INFO
    logging.basicConfig(
        level=logging_level,
        format=config.LOGGING_FORMAT
    )
    logger = logging.getLogger(__name__)
    
    logger.info(f"Opening camera {args.camera}")
    cap = cv2.VideoCapture(args.camera)
    
    if not cap.isOpened():
        logger.error("Error: Could not open camera")
        return
    
    # Set camera properties
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, config.CAMERA_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, config.CAMERA_HEIGHT)
    
    detector = LivenessDetector(config)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            logger.error("Error: Could not read frame")
            break
        
        display_frame, exit_flag = detector.detect_liveness(frame)
        
        cv2.imshow("Liveness Detection", display_frame)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or exit_flag:
            break
        elif key == ord('r'):
            detector.reset()
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()



===== speech_recognizer.py =====

"""Speech recognition module for challenge verification using PocketSphinx."""

import speech_recognition as sr
import threading
import time
import logging
import tempfile
from config import Config
from pocketsphinx import LiveSpeech

class SpeechRecognizer:
    """Handles real-time speech recognition for challenge verification using PocketSphinx."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        self.last_speech = ""
        self.speech_lock = threading.Lock()
        self.last_speech_time = 0
        self.running = False
        self.speech_thread = None
        self.speech_ready = False
        self.target_word = ""  # default target word is empty
        
        try:
            self.logger.info("Initializing PocketSphinx...")
            keywords = config.SPEECH_KEYWORDS
            self.keyword_file = tempfile.NamedTemporaryFile(mode='w', delete=False).name
            with open(self.keyword_file, "w") as f:
                for kw in keywords:
                    f.write(kw + "\n")
            
            self.logger.info(f"Keyword file: {self.keyword_file}")
            
            self.microphone = sr.Microphone()
            with self.microphone as source:
                self.logger.info("Calibrating microphone for ambient noise (0.5s)...")
                sr.Recognizer().adjust_for_ambient_noise(source, duration=0.5)
            
            self.speech = LiveSpeech(
                verbose=False,
                sampling_rate=config.SPEECH_SAMPLING_RATE,
                buffer_size=config.SPEECH_BUFFER_SIZE,
                no_search=False,
                full_utt=False,
                kws=self.keyword_file
            )
            self.speech_ready = True
            self.logger.info("PocketSphinx ready.")
        except ImportError:
            self.logger.error("Could not import pocketsphinx. Please install via pip")
            self.speech_ready = False
        except Exception as e:
            self.logger.error(f"Error initializing pocketsphinx: {e}")
            self.speech_ready = False

    def set_target_word(self, word: str) -> None:
        self.target_word = word.lower().strip()
        self.logger.info(f"Target word set to: {self.target_word}")
    
    def listen_for_speech(self) -> None:
        if not self.speech_ready:
            self.logger.warning("Speech recognition not available")
            return
        
        self.logger.info("Speech recognition thread started")
        self.running = True
        last_detected_word = None
        last_time = 0
        
        try:
            for phrase in self.speech:
                if not self.running:
                    break
                text = str(phrase).lower()
                now = time.time()
                
                # If a target word is set, check if it appears in the phrase.
                if self.target_word and self.target_word in text:
                    with self.speech_lock:
                        self.last_speech = self.target_word
                        self.last_speech_time = now
                    self.logger.info(f"Target word recognized: {self.target_word}")
                    continue

                # Otherwise, process recognized keywords.
                possible_keywords = ["blue", "red", "sky", "ground", "hello", "verify", "noise"]
                first_word = None
                for k in possible_keywords:
                    if k in text:
                        first_word = k
                        break
                if first_word:
                    if first_word == "noise":
                        with self.speech_lock:
                            self.last_speech = ""
                            self.last_speech_time = now
                        last_detected_word = first_word
                        last_time = now
                        self.logger.debug("Detected 'noise' => ignoring")
                    else:
                        # Avoid spamming if the same word repeats in <1s
                        if (first_word != last_detected_word) or ((now - last_time) > 1.0):
                            with self.speech_lock:
                                self.last_speech = first_word
                                self.last_speech_time = now
                            self.logger.info(f"Recognized: {first_word} (full: '{text}')")
                            last_detected_word = first_word
                            last_time = now
                else:
                    self.logger.debug(f"No recognized keyword in '{text}'")
        except Exception as e:
            self.logger.error(f"Error in speech recognition: {e}")
        finally:
            self.running = False
            self.logger.info("Speech recognition thread ended")
    
    def start_listening(self) -> None:
        if not self.speech_ready:
            self.logger.warning("Speech not available")
            return
        if not self.running and (self.speech_thread is None or not self.speech_thread.is_alive()):
            self.speech_thread = threading.Thread(target=self.listen_for_speech, daemon=True)
            self.speech_thread.start()
    
    def stop(self) -> None:
        self.running = False
        if self.speech_thread and self.speech_thread.is_alive():
            self.logger.info("Stopping speech thread...")
            self.speech_thread.join(timeout=1.0)
    
    def get_last_speech(self) -> str:
        with self.speech_lock:
            return self.last_speech
    
    def get_last_speech_time(self) -> float:
        with self.speech_lock:
            return self.last_speech_time
    
    def reset(self) -> None:
        with self.speech_lock:
            self.last_speech = ""
            self.last_speech_time = 0



===== web_app.py =====

"""Web application for liveness detection."""

# Import standard Python libraries for various functionalities
import os  # Handles file system operations like creating directories or removing files
import cv2  # OpenCV library for computer vision tasks, such as image encoding/decoding
import base64  # For encoding binary data (like images) to base64 strings and vice versa
import numpy as np  # Provides numerical operations and array manipulation for image data
import logging  # Enables logging of debug, info, and error messages for troubleshooting
import random  # Used to generate random numbers or selections (e.g., for verification codes)
import string  # Provides string utilities, like digits for generating random codes
import qrcode  # Library to generate QR codes for verification URLs
from flask import Flask, render_template, request, jsonify  # Flask components: app creation, template rendering, HTTP requests, and JSON responses
from flask_socketio import SocketIO, emit, join_room, leave_room  # SocketIO for real-time WebSocket communication between server and clients
import threading  # Allows running background tasks, like cleanup or expiration timers
import time  # Provides time-related functions, such as timestamps and delays
from typing import Dict, Any  # Type hints to clarify dictionary structures and improve code readability

# Import custom project modules
from config import Config  # Loads configuration settings (e.g., timeouts, debug mode) from config.py
from liveness_detector import LivenessDetector  # Main class for liveness detection logic (face, blink, speech, etc.)

# Create Flask application instance
app = Flask(__name__,
            static_folder='static',  # Directory where static files (CSS, JS, images) are served from
            template_folder='templates')  # Directory containing HTML templates for rendering
app.config['SECRET_KEY'] = 'liveness-detection-secret'  # Secret key for securing Flask sessions and CSRF protection

# Initialize SocketIO for real-time communication, allowing connections from any origin
socketio = SocketIO(app, cors_allowed_origins="*")  # "*" allows all domains; adjust for production security

# Instantiate configuration object to access settings
config = Config()

# Configure logging based on debug mode from config
logging_level = logging.DEBUG if config.DEBUG else logging.INFO  # DEBUG logs more details, INFO is less verbose
logging.basicConfig(
    level=logging_level,  # Set the logging level to control verbosity
    format=config.LOGGING_FORMAT  # Use the log format defined in config (e.g., timestamp, level, message)
)
logger = logging.getLogger(__name__)  # Create a logger specific to this module (web_app.py)

# Define global dictionaries to manage application state
active_sessions: Dict[str, Dict[str, Any]] = {}  # Tracks active client sessions by session ID; each session has a detector, code, etc.
verification_codes: Dict[str, Dict[str, Any]] = {}  # Stores verification codes with their status, requester ID, and creation time
last_log_time = {}  # Keeps track of the last time a debug log was emitted per session, for rate-limiting

# Define route for the root URL (homepage)
@app.route('/')
def index():
    # Render the landing page template (index.html) when users visit "/"
    return render_template('index.html')

# Route to verify the code and render a page if it's valid
@app.route('/verify/<code>')
def verify(code):
    # Check if the code is exactly 6 digits
    if not code.isdigit() or len(code) != 6:
        # If not, show error with redirect
        return render_template('error.html', message="Invalid code format", redirect_url="/")

    # Check if the code exists and is still in 'pending' state
    if code not in verification_codes or verification_codes[code]['status'] != 'pending':
        # If not valid or already used/expired, show error
        return render_template('error.html', message="Invalid or expired verification code", redirect_url="/")

    # If everything checks out, render the verification page
    return render_template('verify.html', session_code=code)

# Define route to check if a verification code is valid via HTTP GET
@app.route('/check_code/<code>')
def check_code(code):
    # Log the code check attempt for debugging
    logger.debug(f"Check code route called with code: {code}")
    # Check if the code exists in verification_codes and is still pending
    is_valid = code in verification_codes and verification_codes[code]['status'] == 'pending'
    # Return a JSON response indicating whether the code is valid
    return jsonify({'valid': is_valid})

# Function to clean up a session and its resources when it ends or times out
def cleanup_session(session_id: str, code: str = None):
    """Clean up a session and its associated QR code."""
    # Check if the session exists in active_sessions
    if session_id in active_sessions:
        session_data = active_sessions[session_id]  # Retrieve the session’s data
        # If the session has a detector, stop its speech recognizer to free resources
        if session_data['detector'] is not None:
            session_data['detector'].speech_recognizer.stop()
        # Use provided code or extract it from session data if not provided
        code = code or session_data.get('code')
        # If a code exists and is in verification_codes, clean up associated resources
        if code and code in verification_codes:
            qr_path = f"static/qr_codes/{code}.png"  # Path to the QR code image file
            # Remove the QR code file if it exists on disk
            if os.path.exists(qr_path):
                os.remove(qr_path)
                logger.info(f"Deleted QR code for session {session_id}: {code}")
            # Update the code’s status to 'completed' and remove it from tracking
            verification_codes[code]['status'] = 'completed'
            del verification_codes[code]
        # Remove the session from active_sessions
        del active_sessions[session_id]
        # Log that the cleanup was completed
        logger.info(f"Cleaned up session {session_id} with code {code}")

# SocketIO event handler triggered when a client connects
@socketio.on('connect')
def handle_connect():
    session_id = request.sid  # Get the unique session ID assigned by Flask-SocketIO
    # Log the connection event with the session ID
    logger.info(f"Client connected: {session_id}")

# SocketIO event handler triggered when a client disconnects
@socketio.on('disconnect')
def handle_disconnect():
    session_id = request.sid  # Get the session ID of the disconnecting client
    logger.info(f"Client disconnected: {session_id}")
    # Clean up the session to free resources and remove tracking
    cleanup_session(session_id)

# SocketIO event handler for receiving video frames (older method, using detect_liveness)
@socketio.on('frame')
def handle_frame(data):
    session_id = request.sid  # Get the session ID of the client sending the frame
    # Verify the session exists
    if session_id not in active_sessions:
        logger.warning(f"Received frame from unknown session: {session_id}")
        return  # Exit if session isn’t recognized
    
    # Check if the session has exceeded the maximum allowed attempts (3)
    if active_sessions[session_id].get('attempts', 0) >= 3:
        emit('max_attempts_reached')  # Notify client they’ve hit the limit
        cleanup_session(session_id)  # Clean up the session
        return
    
    # Update the session’s last activity timestamp to track inactivity
    active_sessions[session_id]['last_activity'] = time.time()
    
    try:
        # Extract the base64-encoded image data from the incoming data, skipping the data URI prefix
        image_data = data['image'].split(',')[1]
        # Decode the base64 string into binary data
        image_bytes = base64.b64decode(image_data)
        # Convert binary data into a NumPy array for OpenCV processing
        image_array = np.frombuffer(image_bytes, np.uint8)
        # Decode the array into an image frame using OpenCV
        frame = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        
        # Get the liveness detector instance for this session
        detector = active_sessions[session_id]['detector']
        # Process the frame for liveness detection (older method)
        display_frame, exit_flag = detector.detect_liveness(frame)
        
        # Retrieve the current challenge status from the challenge manager
        challenge_text, action_completed, word_completed, verification_result = \
            detector.challenge_manager.get_challenge_status()
        
        # Encode the processed display frame back to JPEG format
        _, buffer = cv2.imencode('.jpg', display_frame)
        # Convert the JPEG binary data to a base64 string for transmission
        encoded_frame = base64.b64encode(buffer).decode('utf-8')
        
        # Send the processed frame and challenge status back to the client
        emit('processed_frame', {
            'image': f'data:image/jpeg;base64,{encoded_frame}',  # Base64-encoded image with data URI prefix
            'challenge': challenge_text,  # Current challenge text (e.g., "Turn left and say blue")
            'action_completed': action_completed,  # Whether the action part is done
            'word_completed': word_completed,  # Whether the speech part is done
            'time_remaining': detector.challenge_manager.get_challenge_time_remaining(),  # Time left for challenge
            'verification_result': verification_result,  # Result: 'PASS', 'FAIL', or None
            'exit_flag': exit_flag  # Whether to stop processing (True if challenge is complete)
        })
        
        # If the challenge is complete and has a definitive result
        if exit_flag and verification_result in ['PASS', 'FAIL']:
            # Increment the attempt counter for this session
            active_sessions[session_id]['attempts'] = active_sessions[session_id].get('attempts', 0) + 1
            # If the challenge passed or max attempts reached, clean up the session
            if verification_result == 'PASS' or active_sessions[session_id]['attempts'] >= 3:
                cleanup_session(session_id)
    
    except Exception as e:
        # Log any errors that occur during frame processing
        logger.error(f"Error processing frame: {e}")
        # Notify the client of the error
        emit('error', {'message': str(e)})

# SocketIO event handler for resetting a verification session
@socketio.on('reset')
def handle_reset(data):
    session_id = request.sid  # Get the session ID of the client requesting reset
    code = data.get('code')  # Extract the verification code from the data
    
    # Check if the session exists
    if session_id not in active_sessions:
        logger.warning(f"Reset request from unknown session: {session_id}")
        return
    
    try:
        # Get the detector instance and reset its state (e.g., new challenge)
        detector = active_sessions[session_id]['detector']
        detector.reset()
        # Increment the attempt counter
        active_sessions[session_id]['attempts'] = active_sessions[session_id].get('attempts', 0) + 1
        # Log the reset action with the new attempt number
        logger.info(f"Reset detector for session {session_id}, new challenge issued, attempt {active_sessions[session_id]['attempts']}")
        # Confirm the reset to the client
        emit('reset_confirmed')
    except Exception as e:
        # Log any errors during reset
        logger.error(f"Error resetting verification: {e}")
        # Notify the client of the error
        emit('error', {'message': str(e)})

# SocketIO event handler to provide debug status to clients
@socketio.on('get_debug_status')
def handle_get_debug_status():
    # Log the debug status request with current config values
    logger.debug(f"Debug status requested: debug={config.DEBUG}, showDebugFrame={config.SHOW_DEBUG_FRAME}")
    # Send the debug configuration to the client
    emit('debug_status', {
        'debug': config.DEBUG,  # Whether debug mode is enabled
        'showDebugFrame': config.SHOW_DEBUG_FRAME  # Whether to show debug frames with landmarks
    })

# SocketIO event handler to generate a new verification code
@socketio.on('generate_code')
def handle_generate_code():
    session_id = request.sid  # Get the session ID of the requesting client
    logger.info(f"Generate code request from session {session_id}")
    
    # Generate a random 6-digit code using digits 0-9
    code = ''.join(random.choices(string.digits, k=6))
    # Construct the verification URL using the base URL from config
    verification_url = f"{config.BASE_URL}/verify/{code}"
    # Create a QR code object with specified settings
    qr = qrcode.QRCode(version=1, box_size=10, border=5)
    qr.add_data(verification_url)  # Add the URL to the QR code
    qr.make(fit=True)  # Generate the QR code to fit the data
    # Create a black-and-white QR code image
    qr_img = qr.make_image(fill='black', back_color='white')
    qr_path = f"static/qr_codes/{code}.png"  # Define the file path for the QR code image
    qr_img.save(qr_path)  # Save the QR code image to disk
    
    # Store the verification code details in the global dictionary
    verification_codes[code] = {
        'requester_id': session_id,  # ID of the client requesting the code
        'created_at': time.time(),  # Timestamp of code creation
        'status': 'pending'  # Initial status of the code
    }
    
    # Log that the code and QR code are being sent to the client
    logger.info(f"Emitting verification code {code} with QR code to session {session_id}")
    # Send the code and QR code path to the client
    emit('verification_code', {'code': code, 'qr_code': f"/static/qr_codes/{code}.png"})
    
    # Define a function to expire the code after 10 minutes
    def expire_code():
        time.sleep(600)  # Wait 10 minutes (600 seconds)
        # Check if the code still exists and is pending
        if code in verification_codes and verification_codes[code]['status'] == 'pending':
            qr_path = f"static/qr_codes/{code}.png"
            # Remove the QR code file if it exists
            if os.path.exists(qr_path):
                os.remove(qr_path)
                logger.info(f"Deleted QR code for expired code: {code}")
            # Remove the code from tracking
            del verification_codes[code]
            logger.info(f"Expired verification code {code}")
    
    # Start a background thread to handle code expiration
    expiration_thread = threading.Thread(target=expire_code)
    expiration_thread.daemon = True  # Thread will terminate when main program exits
    expiration_thread.start()  # Start the expiration timer

# Background function to periodically clean up inactive sessions
def cleanup_inactive_sessions():
    while True:  # Run indefinitely
        current_time = time.time()  # Get the current timestamp
        inactive_sessions = []  # List to store IDs of sessions to clean up
        
        # Check each session for inactivity
        for session_id, session_data in active_sessions.items():
            # If the session has been inactive longer than the timeout
            if current_time - session_data['last_activity'] > config.SESSION_TIMEOUT:
                inactive_sessions.append(session_id)
        
        # Clean up all identified inactive sessions
        for session_id in inactive_sessions:
            cleanup_session(session_id)
        
        time.sleep(10)  # Wait 10 seconds before the next check

# SocketIO event handler for a client joining a verification session
@socketio.on('join_verification')
def handle_join_verification(data):
    session_id = request.sid  # Get the session ID of the joining client
    code = data.get('code')  # Extract the verification code from the data
    
    # Log the join attempt
    logger.info(f"Client {session_id} joining verification session with code: {code}")
    # Validate the code: must exist, be non-empty, and still pending
    if not code or code not in verification_codes or verification_codes[code]['status'] != 'pending':
        # If invalid, notify the client with an error message
        emit('session_error', {'message': 'Invalid or expired verification code'})
        return
    
    # Update the code’s status to indicate verification is in progress
    verification_codes[code]['status'] = 'in-progress'
    # Store the verifier’s session ID
    verification_codes[code]['verifier_id'] = session_id
    
    # Initialize the session data in active_sessions
    active_sessions[session_id] = {
        'code': code,  # Associate the code with this session
        'detector': None,  # Placeholder for the liveness detector
        'last_activity': time.time(),  # Set initial activity timestamp
        'attempts': 0  # Initialize attempt counter
    }
    
    # Create a new liveness detector instance for this session
    detector = LivenessDetector(config)
    active_sessions[session_id]['detector'] = detector  # Store the detector in session data
    
    join_room(code)  # Add the client to a SocketIO room named after the code
    requester_id = verification_codes[code]['requester_id']  # Get the ID of the original requester
    # Notify the requester that verification has started
    emit('verification_started', {'code': code}, room=requester_id)
    
    # Get the initial challenge text from the detector and send it to the verifier
    challenge_text, _, _, _ = detector.challenge_manager.get_challenge_status()
    emit('challenge', {'text': challenge_text})

# SocketIO event handler for processing video frames (newer method, using process_frame)
@socketio.on('process_frame')
def handle_process_frame(data):
    session_id = request.sid  # Get the session ID of the client sending the frame
    code = data.get('code')  # Extract the verification code from the data
    
    current_time = time.time()  # Get the current timestamp
    # Rate-limit debug logging to once per second per session
    if config.DEBUG and (session_id not in last_log_time or current_time - last_log_time.get(session_id, 0) >= 1.0):
        logger.debug(f"Processing frame for session {session_id}, code {code}")
        last_log_time[session_id] = current_time  # Update last log time
    
    # Validate that the session exists and matches the code
    if session_id not in active_sessions or active_sessions[session_id]['code'] != code:
        logger.warning(f"Received frame from unknown or invalid session: {session_id}, code: {code}")
        emit('session_error', {'message': 'Invalid session or code'})
        return
    
    # Check if the session has exceeded max attempts
    if active_sessions[session_id].get('attempts', 0) >= 3:
        emit('max_attempts_reached')  # Notify client of limit reached
        cleanup_session(session_id, code)  # Clean up the session
        return
    
    # Update the session’s last activity timestamp
    active_sessions[session_id]['last_activity'] = time.time()
    
    try:
        # Decode the base64-encoded image data from the client
        image_data = data['image'].split(',')[1]  # Remove data URI prefix
        image_bytes = base64.b64decode(image_data)  # Convert base64 to binary
        nparr = np.frombuffer(image_bytes, np.uint8)  # Create NumPy array from binary data
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)  # Decode into an OpenCV image
        logger.debug(f"Frame decoded: shape={frame.shape if frame is not None else 'None'}")
        
        # Process the frame using the session’s liveness detector
        detector = active_sessions[session_id]['detector']
        result = detector.process_frame(frame)  # Get detailed processing results
        
        # Extract display and debug frames from the result
        display_frame = result['display_frame']
        debug_frame = result['debug_frame']
        
        # Encode the display frame to base64 if it exists
        if display_frame is not None:
            _, buffer_disp = cv2.imencode('.jpg', display_frame)  # Convert to JPEG
            disp_b64 = base64.b64encode(buffer_disp).decode('utf-8')  # Encode to base64 string
            logger.debug("Display frame encoded")
        else:
            disp_b64 = None  # No display frame available
            logger.debug("Display frame is None")
        
        # Encode the debug frame to base64 if it exists
        debug_b64 = None
        if debug_frame is not None:
            _, buffer_dbg = cv2.imencode('.jpg', debug_frame)  # Convert to JPEG
            debug_b64 = base64.b64encode(buffer_dbg).decode('utf-8')  # Encode to base64 string
            logger.debug("Debug frame encoded")
        else:
            logger.debug("Debug frame is None")
        
        # Prepare the data packet to send back to the client
        emit_data = {
            'image': f"data:image/jpeg;base64,{disp_b64}" if disp_b64 else None,  # Display frame or None
            'debug_image': f"data:image/jpeg;base64,{debug_b64}" if debug_b64 else None,  # Debug frame or None
            'challenge': result['challenge_text'],  # Current challenge instruction
            'action_completed': result['action_completed'],  # Action status (True/False)
            'word_completed': result['word_completed'],  # Speech status (True/False)
            'time_remaining': result['time_remaining'],  # Time left for challenge
            'verification_result': result['verification_result'],  # 'PENDING', 'PASS', or 'FAIL'
            'exit_flag': result['exit_flag'],  # Whether to stop processing
            'duress_detected': result['duress_detected']  # Whether duress was detected
        }
        # Log the data being emitted for debugging
        logger.debug(f"Emitting processed_frame: has_image={bool(disp_b64)}, has_debug={bool(debug_b64)}")
        emit('processed_frame', emit_data)  # Send the data to the client
        
        # Handle the outcome if the challenge is complete
        if result['exit_flag']:
            # Increment the attempt counter
            active_sessions[session_id]['attempts'] = active_sessions[session_id].get('attempts', 0) + 1
            logger.debug(f"Attempt {active_sessions[session_id]['attempts']} for session {session_id}")
            requester_id = verification_codes[code]['requester_id']  # Get the requester’s ID
            
            # If duress is detected, notify requester and clean up
            if result['duress_detected']:
                emit('verification_result', {
                    'result': 'FAIL',
                    'code': code,
                    'duress_detected': True
                }, room=requester_id)
                cleanup_session(session_id, code)
            # If verification passes, notify requester and clean up
            elif result['verification_result'] == 'PASS':
                emit('verification_result', {
                    'result': 'PASS',
                    'code': code,
                    'duress_detected': False
                }, room=requester_id)
                cleanup_session(session_id, code)
            # If verification fails or times out
            elif result['verification_result'] == 'FAIL' or result['time_remaining'] <= 0:
                # If max attempts reached, notify requester and clean up
                if active_sessions[session_id]['attempts'] >= 3:
                    emit('verification_result', {
                        'result': 'FAIL',
                        'code': code,
                        'duress_detected': False
                    }, room=requester_id)
                    cleanup_session(session_id, code)
                else:
                    # Reset the detector for another attempt and send new challenge
                    detector.reset()
                    logger.info(f"Reset detector after failure/timeout for session {session_id}, attempt {active_sessions[session_id]['attempts']}")
                    emit('challenge', {'text': detector.challenge_manager.get_challenge_status()[0]})
    except Exception as e:
        # Log any errors during frame processing
        logger.error(f"Error processing frame: {e}")
        # Notify the client of the error
        emit('error', {'message': str(e)})

# SocketIO event handler for when verification is explicitly completed
@socketio.on('verification_complete')
def handle_verification_complete(data):
    code = data.get('code')  # Get the verification code from the data
    result = data.get('result')  # Get the result ('PASS' or 'FAIL')
    
    # Check if the code exists in verification_codes
    if code and code in verification_codes:
        requester_id = verification_codes[code]['requester_id']  # Get the requester’s ID
        # Notify the requester of the verification result
        emit('verification_result', {
            'result': result,
            'code': code
        }, room=requester_id)
        # Clean up all sessions associated with this code
        for session_id, session_data in list(active_sessions.items()):
            if session_data.get('code') == code:
                cleanup_session(session_id, code)
        # Log the completion of the verification
        logger.info(f"Verification {code} completed with result: {result}")

# Main execution block, runs if the script is executed directly
if __name__ == '__main__':
    # Create the QR code directory if it doesn’t exist
    os.makedirs('static/qr_codes', exist_ok=True)
    # Start a background thread to clean up inactive sessions
    cleanup_thread = threading.Thread(target=cleanup_inactive_sessions)
    cleanup_thread.daemon = True  # Thread will stop when the main program exits
    cleanup_thread.start()  # Begin the cleanup loop
    
    # Launch the Flask-SocketIO server with configured settings
    socketio.run(
        app,  # Flask application instance
        host=config.HOST,  # Host address (e.g., '0.0.0.0' to listen on all interfaces)
        port=config.PORT,  # Port number (e.g., 8080) from config
        debug=config.DEBUG,  # Enable debug mode if True in config
        certfile=config.CERTFILE,  # Path to SSL certificate file for HTTPS
        keyfile=config.KEYFILE  # Path to SSL private key file for HTTPS
    )


===== action_detector.py =====

"""Action detection module for liveness verification."""

import cv2
import numpy as np
import logging
import dlib
from typing import List, Tuple, Dict, Any, Optional
from collections import deque

class ActionDetector:
    """Detects specific actions for liveness verification."""
    
    def __init__(self, config):
        """Initialize the action detector with configuration."""
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load dlib face detector and shape predictor
        self.dlib_detector = dlib.get_frontal_face_detector()
        try:
            self.dlib_predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
            self.using_dlib = True
            self.logger.info("Using dlib for facial landmark detection in ActionDetector")
        except Exception as e:
            self.logger.error(f"Could not load dlib shape predictor: {e}")
            self.using_dlib = False
            raise ValueError("Dlib shape predictor is required for action detection")
        
        # Action detection variables
        self.current_action = None
        self.action_completed = False
        self.action_start_time = None
        
        # Head pose tracking
        self.face_positions = deque(maxlen=config.FACE_POSITION_HISTORY_LENGTH)
        self.face_angles = deque(maxlen=config.FACE_POSITION_HISTORY_LENGTH)
        self.head_pose = "center"  # center, left, right, up, down

        # [CHANGED] Rate-limit debug logs
        self.last_debug_time = 0.0
    
    def set_action(self, action: str) -> None:
        """
        Set the current action to detect.
        
        Args:
            action: Action name ("left", "right", "up", "down")
        """
        self.current_action = action
        self.action_completed = False
        self.action_start_time = None
        self.logger.debug(f"Action set to: {action}")
    
    def detect_head_pose(self, frame: np.ndarray, face_rect: Tuple[int, int, int, int]) -> str:
        """
        Detect head pose (left, right, up, down, center).
        """
        if face_rect is None:
            return self.head_pose
        
        x, y, w, h = face_rect
        
        face_center_x = x + w/2
        frame_center_x = frame.shape[1] / 2
        face_center_y = y + h/2
        frame_center_y = frame.shape[0] / 2
        
        x_offset = face_center_x - frame_center_x
        y_offset = face_center_y - frame_center_y
        
        x_offset_normalized = x_offset / (frame.shape[1] / 2)
        y_offset_normalized = y_offset / (frame.shape[0] / 2)
        
        self.face_angles.append((x_offset_normalized, y_offset_normalized))
        
        if len(self.face_angles) >= 5:
            angles_list = list(self.face_angles)
            avg_x_offset = sum(a[0] for a in angles_list) / len(angles_list)
            avg_y_offset = sum(a[1] for a in angles_list) / len(angles_list)
            
            x_threshold = self.config.HEAD_POSE_THRESHOLD_X
            y_threshold_up = self.config.HEAD_POSE_THRESHOLD_Y_UP
            y_threshold_down = self.config.HEAD_POSE_THRESHOLD_Y_DOWN
            
            old_pose = self.head_pose
            
            if avg_x_offset < -x_threshold:
                self.head_pose = "right"
            elif avg_x_offset > x_threshold:
                self.head_pose = "left"
            elif avg_y_offset < -y_threshold_up:
                self.head_pose = "up"
            elif avg_y_offset > y_threshold_down:
                self.head_pose = "down"
            else:
                self.head_pose = "center"
            
            now = float(cv2.getTickCount()) / cv2.getTickFrequency()
            if self.head_pose != old_pose and now - self.last_debug_time > 1.0:
                self.logger.debug(f"{self.head_pose.upper()} detected!")
                self.last_debug_time = now
            
            # Draw direction indicator for debugging
            center_x = int(frame.shape[1] / 2)
            center_y = int(frame.shape[0] / 2)
            direction_x = int(center_x + avg_x_offset * 100)
            direction_y = int(center_y + avg_y_offset * 100)
            cv2.line(frame, (center_x, center_y), (direction_x, direction_y), (0, 255, 255), 2)
        
        return self.head_pose
    
    def detect_action(self, frame: np.ndarray, face_rect: Tuple[int, int, int, int]) -> bool:
        """
        Detect the specified action.
        """
        if self.current_action is None or face_rect is None:
            return False
        
        current_pose = self.detect_head_pose(frame, face_rect)
        self.action_completed = (current_pose.lower() == self.current_action.lower())
        return self.action_completed
    
    def is_action_completed(self):
        """Check if the current action is completed."""
        return self.action_completed



===== static/css/style.css =====

* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #e0e0e0;
    background-color: #121212;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1 {
    text-align: center;
    margin-bottom: 30px;
    color: #ffffff;
    font-weight: 300;
    letter-spacing: 1px;
}

h2 {
    color: #ffffff;
    font-weight: 400;
    margin-bottom: 15px;
    letter-spacing: 0.5px;
}

h3 {
    color: #ffffff;
    font-weight: 400;
    margin-bottom: 10px;
    letter-spacing: 0.5px;
}

/* Card layout for landing page */
.card-container {
    display: flex;
    justify-content: center;
    gap: 30px;
    margin-bottom: 40px;
}

.card {
    background-color: #1e1e1e;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    width: 100%;
    max-width: 450px;
    border: 1px solid #333;
    transition: transform 0.3s, box-shadow 0.3s;
}

.card:hover {
    transform: translateY(-5px);
    box-shadow: 0 15px 30px rgba(0, 0, 0, 0.4);
}

.card p {
    color: #bbb;
    margin-bottom: 20px;
}

.input-group {
    display: flex;
    margin-top: 20px;
}

input[type="text"] {
    flex: 1;
    padding: 12px 15px;
    border: 1px solid #333;
    background-color: #252525;
    color: #fff;
    border-radius: 30px 0 0 30px;
    font-size: 16px;
    outline: none;
    transition: border-color 0.3s;
}

input[type="text"]:focus {
    border-color: #3498db;
}

.input-group .button {
    border-radius: 0 30px 30px 0;
}

.verification-code {
    font-size: 36px;
    font-weight: bold;
    letter-spacing: 5px;
    color: #3498db;
    background-color: #252525;
    padding: 15px;
    border-radius: 8px;
    margin: 15px 0;
    text-align: center;
}

#qr-display img {
    margin-top: 15px;
    max-width: 200px;
    border-radius: 8px;
    border: 1px solid #333;
    object-fit: contain;
    display: block;
    margin: 0 auto;
}

.status-waiting {
    padding: 10px;
    color: #3498db;
    background-color: rgba(73, 100, 251, 0.2);
    border: 1px solid rgba(73, 100, 251, 0.3);
    border-radius: 8px;
    font-weight: 500;
    margin-top: 20px;
    margin-bottom: 20px;
    text-align: center;
    font-size: 20px;
    transition: all 0.3s ease;
}

.status-progress {
    padding: 10px;
    color: #e0bd65;
    background-color: rgba(248, 255, 48, 0.2); 
    border: 1px solid rgba(248, 255, 48, 0.3);
    border-radius: 8px;
    font-weight: 500;
    margin-top: 20px;
    margin-bottom: 20px;
    text-align: center;
    font-size: 20px;
    transition: all 0.3s ease;
}

.status-success {
    padding: 10px;
    color: #4cd137;
    background-color: rgba(46, 204, 113, 0.2);
    border: 1px solid rgba(46, 204, 113, 0.3);
    border-radius: 8px;
    font-weight: 500;
    margin-top: 20px;
    margin-bottom: 20px;
    text-align: center;
    font-size: 20px;
    transition: all 0.3s ease;
}

.status-failed {
    padding: 10px;
    color: #e8603e;
    background-color: rgba(231, 76, 60, 0.2);   
    border: 1px solid rgba(231, 76, 60, 0.3);
    border-radius: 8px;
    font-weight: 500;
    margin-top: 20px;
    margin-bottom: 20px;
    text-align: center;
    font-size: 20px;
    transition: all 0.3s ease;
}

.status-duress {
    padding: 10px;
    color: #ff0000;
    background-color: rgba(231, 76, 60, 0.2);
    border: 1px solid rgba(231, 76, 60, 0.3);
    border-radius: 8px;
    font-weight: 500;
    margin-top: 20px;
    margin-bottom: 20px;
    text-align: center;
    font-size: 20px;
    white-space: pre-line;
    transition: all 0.3s ease;
}

.session-info {
    background-color: #1e1e1e;
    padding: 10px 20px;
    border-radius: 30px;
    display: inline-block;
    margin: 0 auto 20px;
    text-align: center;
    border: 1px solid #333;
}

.session-info p {
    margin: 0;
}

#session-code {
    font-weight: bold;
    color: #3498db;
    letter-spacing: 1px;
}

.footer {
    text-align: center;
    margin-top: 40px;
    color: #666;
    font-size: 14px;
}

/* Video container styles */
.video-container {
    display: flex;
    justify-content: space-between;
    margin-bottom: 20px;
    gap: 20px;
}

.video-container.single-video {
    justify-content: center;
}

.video-wrapper {
    position: relative;
    width: 48%;
    max-width: 640px;
    border-radius: 12px;
    overflow: hidden;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
}

.single-video .video-wrapper {
    width: 100%;
    max-width: 640px;
}

#webcam, #overlay {
    width: 100%;
    height: auto;
    border-radius: 12px;
}

#overlay {
    position: absolute;
    top: 0;
    left: 0;
}

#processed-frame-container {
    width: 48%;
}

#processed-frame {
    width: 100%;
    height: auto;
    border-radius: 12px;
    border: 1px solid #333;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
}

.challenge-container {
    background-color: #1e1e1e;
    padding: 20px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    margin-bottom: 20px;
    border: 1px solid #333;
}

.challenge-text {
    font-size: 24px;
    font-weight: 500;
    text-align: center;
    margin-bottom: 15px;
    color: #ffffff;
    letter-spacing: 0.5px;
}

.status-container {
    display: flex;
    justify-content: space-around;
    background-color: #252525;
    padding: 15px;
    border-radius: 8px;
}

.status-item {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-size: 18px;
}

.status-item span:first-child {
    color: #999;
    margin-bottom: 5px;
    font-size: 14px;
    text-transform: uppercase;
    letter-spacing: 1px;
}

.result-container {
    background-color: #1e1e1e;
    padding: 25px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    margin-bottom: 20px;
    text-align: center;
    border: 1px solid #333;
}

.result-text {
    font-size: 1.5rem;
    font-weight: bold;
    text-align: center;
    padding: 1rem;
    border-radius: 5px;
    margin-bottom: 1rem;
}

.result-text.success {
    background-color: rgba(46, 204, 113, 0.3);
    color: #27ae60;
}

.result-text.failure {
    background-color: rgba(231, 76, 60, 0.3);
    color: #c0392b;
}

.result-text.duress {
    background-color: rgba(230, 126, 34, 0.3);
    color: #e67e22;
}

.button {
    background-color: #2980b9;
    color: white;
    border: none;
    padding: 12px 25px;
    font-size: 16px;
    border-radius: 30px;
    cursor: pointer;
    transition: all 0.3s;
    text-transform: uppercase;
    letter-spacing: 1px;
    font-weight: 500;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

.button:hover {
    background-color: #3498db;
    transform: translateY(-2px);
    box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15);
}

.button:active {
    transform: translateY(0);
}

.instructions {
    background-color: #1e1e1e;
    padding: 20px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    border: 1px solid #333;
}

.instructions h2 {
    margin-bottom: 15px;
    color: #ffffff;
    font-weight: 400;
    letter-spacing: 0.5px;
}

.instructions p {
    color: #bbb;
    margin-bottom: 10px;
    padding-left: 15px;
    position: relative;
}

.instructions p:before {
    content: "•";
    position: absolute;
    left: 0;
    color: #3498db;
}

.hidden {
    display: none;
}

@media (max-width: 768px) {
    .card-container {
        flex-direction: column;
        align-items: center;
    }
    
    .video-container {
        flex-direction: column;
    }
    
    .video-wrapper, #processed-frame-container {
        width: 100%;
    }
    
    .challenge-text {
        font-size: 20px;
    }
    
    .status-item {
        font-size: 16px;
    }
}

.error-container {
    background-color: #1e1e1e;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    text-align: center;
    border: 1px solid #333;
    margin: 50px auto;
    max-width: 500px;
}

.error-icon {
    font-size: 68px;
    color: #e74c3c;
    margin-bottom: 20px;
}

.error-message {
    font-size: 24px;
    color: #ffffff;
    margin-bottom: 30px;
}

.verify-status {
    margin-top: 15px;
    padding: 10px;
    border-radius: 8px;
    text-align: center;
    font-weight: 500;
    transition: all 0.3s ease;
    height: 40px;
    display: flex;
    align-items: center;
    justify-content: center;
}

.verify-status.error {
    background-color: rgba(231, 76, 60, 0.2);
    color: #e74c3c;
    border: 1px solid rgba(231, 76, 60, 0.3);
}

.verify-status.success {
    background-color: rgba(46, 204, 113, 0.2);
    color: #2ecc71;
    border: 1px solid rgba(46, 204, 113, 0.3);
}

.verify-status.info {
    background-color: rgba(52, 152, 219, 0.2);
    color: #3498db;
    border: 1px solid rgba(52, 152, 219, 0.3);
}

/* Success/Failure/Duress overlay effects */
.success-overlay {
    box-shadow: 0 0 20px 10px rgba(46, 204, 113, 0.6);
    filter: sepia(0.2) saturate(1.5) brightness(1.1) hue-rotate(60deg);
    border: 3px solid #2ecc71;
}

.failure-overlay {
    box-shadow: 0 0 20px 10px rgba(231, 76, 60, 0.6);
    filter: sepia(0.3) saturate(1.5) brightness(0.9) hue-rotate(-20deg);
    border: 3px solid #e74c3c;
}

.duress-overlay {
    box-shadow: 0 0 20px 10px rgba(230, 126, 34, 0.6);
    filter: sepia(0.3) saturate(1.5) brightness(0.9) hue-rotate(10deg);
    border: 3px solid #e67e22;
}

/* Debug frame styles */
#debug-frame {
    width: 100%;
    height: auto;
    border-radius: 12px;
    border: 1px solid #333;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    object-fit: contain;
}

#processed-frame-container, .video-wrapper {
    width: 48%;
    max-width: 640px;
    height: auto;
}


===== templates/error.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error - Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Error</h1>
        <div class="error-container">
            <div class="error-icon">⛔</div>
            <div class="error-message">{{ message }}</div>
            <a href="{{ redirect_url }}" class="button">Return to Home</a>
        </div>
    </div>
    
    <script>
        // Automatically redirect after 5 seconds
        setTimeout(() => {
            window.location.href = "{{ redirect_url }}";
        }, 5000);
    </script>
</body>
</html> 


===== templates/index.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Liveness Verification System</h1>
        
        <div class="card-container">
            <div class="card">
                <h2>Request Verification</h2>
                <p>Generate a one-time code and QR code to verify someone’s identity</p>
                <div id="request-container">
                    <button id="generate-code-btn" class="button">Generate Code</button>
                    <div id="code-display" class="hidden">
                        <h3>Your verification code:</h3>
                        <div class="verification-code"><span id="verification-code"></span></div>
                        <div id="qr-display"></div> <!-- QR code display -->
                        <p>Share this code or scan the QR code to verify (expires in 10 minutes)</p>
                        <div id="verification-status" class="status-waiting">
                            Waiting for verification...
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2>Complete Verification</h2>
                <p>Enter a verification code to prove you're human</p>
                <div id="verify-container">
                    <div class="input-group">
                        <input type="text" id="code-input" placeholder="Enter 6-digit code" maxlength="6" pattern="[0-9]{6}">
                        <button id="submit-code-btn" class="button">Verify</button>
                    </div>
                    <!-- verify-status will be added here by JavaScript -->
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>Secure liveness detection for remote identity verification</p>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <script src="{{ url_for('static', filename='js/landing.js') }}"></script>
</body>
</html>


===== templates/verify.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Liveness Verification</h1>
        <div class="session-info">
            <p>Session Code: <span id="session-code">{{ session_code }}</span></p>
        </div>
        
        <div class="video-container">
            <div class="video-wrapper">
                <video id="webcam" autoplay playsinline muted crossOrigin="anonymous"></video>
                <canvas id="overlay" style="display: none;"></canvas>
            </div>
            <div id="processed-frame-container">
                <img id="debug-frame" alt="Debug Frame">
            </div>
        </div>
        
        <div class="challenge-container">
            <div id="challenge-text" class="challenge-text">Waiting for challenge...</div>
            <div class="status-container">
                <div class="status-item">
                    <span>Action</span>
                    <span id="action-status">❌</span>
                </div>
                <div class="status-item">
                    <span>Word</span>
                    <span id="word-status">❌</span>
                </div>
                <div class="status-item">
                    <span>Time</span>
                    <span id="time-remaining">0s</span>
                </div>
            </div>
        </div>
        
        <div id="result-container" class="result-container hidden">
            <div id="result-text" class="result-text"></div>
            <button id="reset-button" class="button">Try Again</button>
        </div>
        
        <div class="instructions">
            <h2>Instructions</h2>
            <p>Allow camera access when prompted</p>
            <p>Follow the challenge instructions displayed on screen</p>
            <p>Complete both the action and speech parts of the challenge</p>
            <p>You have up to 3 attempts to verify your identity</p>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <script src="{{ url_for('static', filename='js/app.js') }}"></script>
</body>
</html>


===== static/js/app.js =====

document.addEventListener('DOMContentLoaded', () => {
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const debugFrame = document.getElementById('debug-frame');
    const challengeText = document.getElementById('challenge-text');
    const actionStatus = document.getElementById('action-status');
    const wordStatus = document.getElementById('word-status');
    const timeRemaining = document.getElementById('time-remaining');
    const resultContainer = document.getElementById('result-container');
    const resultText = document.getElementById('result-text');
    const resetButton = document.getElementById('reset-button');
    const videoContainer = document.querySelector('.video-container');
    const processedFrameContainer = document.getElementById('processed-frame-container');
    const sessionCode = document.getElementById('session-code').textContent;
    
    let socket;
    let isProcessing = false;
    let stream = null;
    let verificationAttempts = 0;
    const MAX_VERIFICATION_ATTEMPTS = 3;
    let isDebugMode = true;
    let showDebugFrame = true;
    let frameCount = 0;
    
    video.muted = true;
    video.volume = 0;

    function initSocket() {
        console.log('Initializing socket connection for verification');
        socket = io();
        
        socket.on('connect', () => {
            console.log('Connected to server for verification');
            socket.emit('join_verification', { code: sessionCode });
            socket.emit('get_debug_status');
        });
        
        socket.on('debug_status', (data) => {
            isDebugMode = data.debug;
            showDebugFrame = data.showDebugFrame;
            console.log(`Debug mode: ${isDebugMode}, Show debug frame: ${showDebugFrame}`);
            
            // Set visibility based on server config
            if (showDebugFrame) {
                processedFrameContainer.classList.remove('hidden');
                debugFrame.classList.remove('hidden');
                videoContainer.classList.remove('single-video');
            } else {
                processedFrameContainer.classList.add('hidden');
                debugFrame.classList.add('hidden');
                videoContainer.classList.add('single-video');
            }
            
            isProcessing = true;
            console.log('Processing started after debug status received');
            requestAnimationFrame(captureAndSendFrame);
        });
        
        socket.on('processed_frame', (data) => {
            if (isDebugMode && frameCount % 30 === 0) {
                console.log('Received processed_frame:', {
                    hasImage: !!data.image,
                    hasDebugImage: !!data.debug_image,
                    challenge: data.challenge,
                    action: data.action_completed,
                    word: data.word_completed,
                    time: data.time_remaining,
                    result: data.verification_result,
                    duress: data.duress_detected
                });
            }
            
            // Handle debug frame visibility
            if (showDebugFrame && data.debug_image) {
                debugFrame.src = data.debug_image;
                processedFrameContainer.classList.add('visible');
                videoContainer.classList.add('double-video');
            } else if (data.image) {
                debugFrame.src = data.image; // Fallback for non-debug mode
                if (!showDebugFrame) {
                    processedFrameContainer.classList.remove('visible');
                    videoContainer.classList.remove('double-video');
                }
            }
        
            if (data.challenge) {
                challengeText.textContent = data.challenge;
            } else {
                challengeText.textContent = 'Waiting for challenge...';
            }
            
            actionStatus.textContent = data.action_completed ? '✅' : '❌';
            wordStatus.textContent = data.word_completed ? '✅' : '❌';
            
            if (data.time_remaining !== undefined) {
                timeRemaining.textContent = Math.max(0, Math.ceil(data.time_remaining)) + 's';
            }
            
            if (data.verification_result !== 'PENDING') {
                isProcessing = false; // Stop processing new frames
                
                // Freeze the video by pausing and capturing the last frame
                video.pause();
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                canvas.style.display = 'block'; // Show canvas over video
                
                // Overlay result text
                let resultMessage, textColor;
                if (data.duress_detected) {
                    resultMessage = 'Duress Detected!';
                    textColor = '#ff0000';
                    applyVideoEffect('duress');
                } else if (data.verification_result === 'PASS') {
                    resultMessage = 'Verification Successful!';
                    textColor = '#4cd137';
                    applyVideoEffect('success');
                } else {
                    resultMessage = 'Verification Failed!';
                    textColor = '#e8603e';
                    applyVideoEffect('failure');
                }
                
                ctx.fillStyle = 'rgba(0, 0, 0, 0.7)'; // Semi-transparent background
                ctx.fillRect(0, canvas.height / 2 - 40, canvas.width, 80);
                ctx.fillStyle = textColor;
                ctx.font = 'bold 30px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(resultMessage, canvas.width / 2, canvas.height / 2 + 10);
                
                resultText.textContent = resultMessage;
                resultText.className = `result-text ${data.duress_detected ? 'duress' : data.verification_result === 'PASS' ? 'success' : 'failure'}`;
                resultContainer.classList.remove('hidden');
                
                verificationAttempts++;
                console.log(`Attempt ${verificationAttempts} of ${MAX_VERIFICATION_ATTEMPTS}`);
                
                if (data.verification_result === 'PASS' || data.duress_detected || verificationAttempts >= MAX_VERIFICATION_ATTEMPTS) {
                    setTimeout(() => window.location.href = '/', 5000);
                } else {
                    setTimeout(() => {
                        isProcessing = true;
                        canvas.style.display = 'none';
                        video.play();
                        removeVideoEffect();
                        requestAnimationFrame(captureAndSendFrame);
                    }, 3000);
                }
            } else if (isProcessing) {
                requestAnimationFrame(captureAndSendFrame);
            }
            
            if (data.time_remaining <= 0 && data.verification_result === 'PENDING') {
                socket.emit('reset', { code: sessionCode });
                verificationAttempts++;
                
                if (verificationAttempts >= MAX_VERIFICATION_ATTEMPTS) {
                    resultText.textContent = 'Maximum attempts reached. Verification failed.';
                    resultText.className = 'result-text failure';
                    resultContainer.classList.remove('hidden');
                    isProcessing = false;
                    applyVideoEffect('failure');
                    setTimeout(() => window.location.href = '/', 5000);
                } else {
                    challengeText.textContent = `Attempt ${verificationAttempts + 1} of ${MAX_VERIFICATION_ATTEMPTS}...`;
                    setTimeout(() => {
                        challengeText.textContent = 'Waiting for new challenge...';
                        isProcessing = true;
                        requestAnimationFrame(captureAndSendFrame);
                    }, 2000);
                }
            }
        });
        
        socket.on('error', (data) => {
            console.error('Server error:', data.message);
            alert('Server error: ' + data.message);
        });
        
        socket.on('max_attempts_reached', () => {
            isProcessing = false;
            resultText.textContent = 'Maximum verification attempts reached.';
            resultContainer.classList.remove('hidden');
            applyVideoEffect('failure');
            setTimeout(() => window.location.href = '/', 5000);
        });
        
        socket.on('disconnect', () => {
            console.log('Disconnected from server');
            isProcessing = false;
        });
        
        socket.on('challenge', (data) => {
            if (data && data.text) {
                challengeText.textContent = data.text;
            }
        });
    }
    
    function applyVideoEffect(effect) {
        if (effect === 'success') {
            videoContainer.classList.add('success-overlay');
        } else if (effect === 'failure') {
            videoContainer.classList.add('failure-overlay');
        } else if (effect === 'duress') {
            videoContainer.classList.add('duress-overlay');
        }
    }
    
    function removeVideoEffect() {
        videoContainer.classList.remove('success-overlay', 'failure-overlay', 'duress-overlay');
    }
    
    function captureAndSendFrame() {
        if (!isProcessing) {
            console.log('Processing stopped, not capturing frame');
            return;
        }
        
        try {
            const offscreenCanvas = document.createElement('canvas');
            offscreenCanvas.width = video.videoWidth;
            offscreenCanvas.height = video.videoHeight;
            const offscreenCtx = offscreenCanvas.getContext('2d');
            offscreenCtx.drawImage(video, 0, 0, offscreenCanvas.width, offscreenCanvas.height);
            const imageData = offscreenCanvas.toDataURL('image/jpeg', 0.8);
            
            if (isDebugMode && frameCount % 30 === 0) {
                console.log(`Sending frame #${frameCount}`);
            }
            
            socket.emit('process_frame', {
                image: imageData,
                code: sessionCode
            });
            frameCount++;
        } catch (err) {
            console.error('Error capturing frame:', err);
        }
    }
    
    async function initWebcam() {
        try {
            stream = await navigator.mediaDevices.getUserMedia({
                video: {
                    width: { ideal: 640 },
                    height: { ideal: 480 },
                    facingMode: 'user'
                },
                audio: true
            });
            video.srcObject = stream;
            video.onloadedmetadata = () => {
                video.play().catch(err => console.error('Error playing video:', err));
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                requestAnimationFrame(captureAndSendFrame);
            };
        } catch (err) {
            console.error('Error accessing webcam:', err);
            alert('Error accessing webcam: ' + err.message);
        }
    }
    
    resetButton.addEventListener('click', () => {
        if (socket && socket.connected) {
            socket.emit('reset', { code: sessionCode });
            isProcessing = true;
            removeVideoEffect();
            requestAnimationFrame(captureAndSendFrame);
        }
    });
    
    function init() {
        initSocket();
        initWebcam();
    }
    
    window.addEventListener('beforeunload', () => {
        if (socket && socket.connected) {
            socket.disconnect();
        }
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
        }
    });
    
    init();
});


===== static/js/landing.js =====

document.addEventListener('DOMContentLoaded', () => {
    const generateCodeBtn = document.getElementById('generate-code-btn');
    const codeDisplay = document.getElementById('code-display');
    const verificationCode = document.getElementById('verification-code');
    const verificationStatus = document.getElementById('verification-status');
    const codeInput = document.getElementById('code-input');
    const submitCodeBtn = document.getElementById('submit-code-btn');
    const verifyContainer = document.getElementById('verify-container');
    const verifyStatus = document.createElement('div');
    const qrDisplay = document.createElement('div'); // Add QR code display
    
    verifyStatus.className = 'verify-status';
    verifyContainer.appendChild(verifyStatus);
    qrDisplay.id = 'qr-display';
    codeDisplay.appendChild(qrDisplay); // Add QR display under code
    
    let socket;
    let currentCode = null;
    
    // Initialize Socket.IO connection
    function initSocket() {
        console.log('Initializing socket connection');
        socket = io();
        
        socket.on('connect', () => {
            console.log('Connected to server');
        });
        
        socket.on('disconnect', () => {
            console.log('Disconnected from server');
        });
        
        socket.on('error', (data) => {
            console.error('Socket error:', data);
            verificationStatus.textContent = `Error: ${data.message}`;
            verificationStatus.className = 'status-error';
        });
        
        socket.on('verification_code', (data) => {
            console.log('Received verification code:', data.code);
            currentCode = data.code;
            verificationCode.textContent = currentCode;
            codeDisplay.classList.remove('hidden');
            generateCodeBtn.classList.add('hidden');
            qrDisplay.innerHTML = `<img src="${data.qr_code}" alt="QR Code for ${data.code}">`;
            
            // Update status
            verificationStatus.textContent = 'Waiting for verification...';
            verificationStatus.className = 'status-waiting';
        });
        
        socket.on('verification_started', (data) => {
            if (data.code === currentCode) {
                verificationStatus.textContent = 'Verification in progress...';
                verificationStatus.className = 'status-progress';
            }
        });
        
        socket.on('verification_result', (data) => {
            if (data.code === currentCode) {
                if (data.duress_detected) {
                    verificationStatus.textContent = 'Duress Detected!\n!!! DO NOT PROCEED !!!';
                    verificationStatus.className = 'status-duress';
                } else if (data.result === 'PASS') {
                    verificationStatus.textContent = 'Verification PASSED';
                    verificationStatus.className = 'status-success';
                } else {
                    verificationStatus.textContent = 'Verification FAILED';
                    verificationStatus.className = 'status-failed';
                }
                // Status persists; no auto-reset here
            }
        });
        
        socket.on('code_error', (data) => {
            showVerifyError(data.message);
        });
    }
    
    // Generate verification code
    generateCodeBtn.addEventListener('click', () => {
        console.log('Generate code button clicked');
        if (socket && socket.connected) {
            socket.emit('generate_code');
        } else {
            console.error('Socket not connected');
            initSocket();
            setTimeout(() => {
                if (socket && socket.connected) {
                    socket.emit('generate_code');
                } else {
                    console.error('Failed to reconnect socket');
                }
            }, 1000);
        }
    });
    
    // Show verification error
    function showVerifyError(message) {
        verifyStatus.textContent = message;
        verifyStatus.className = 'verify-status error';
        
        setTimeout(() => {
            verifyStatus.textContent = '';
            verifyStatus.className = 'verify-status';
        }, 3000);
    }
    
    // Show verification success
    function showVerifySuccess(message) {
        verifyStatus.textContent = message;
        verifyStatus.className = 'verify-status success';
        
        setTimeout(() => {
            verifyStatus.textContent = '';
            verifyStatus.className = 'verify-status';
        }, 1000);
    }
    
    // Submit verification code
    submitCodeBtn.addEventListener('click', () => {
        const code = codeInput.value.trim();
        
        if (code.length !== 6 || !/^\d+$/.test(code)) {
            showVerifyError('Please enter a valid 6-digit code');
            return;
        }
        
        submitCodeBtn.disabled = true;
        submitCodeBtn.textContent = 'Checking...';
        verifyStatus.textContent = 'Validating code...';
        verifyStatus.className = 'verify-status info';
        
        fetch(`/check_code/${code}`)
            .then(response => {
                if (!response.ok) {
                    throw new Error(`Server error: ${response.status}`);
                }
                return response.json();
            })
            .then(data => {
                submitCodeBtn.disabled = false;
                submitCodeBtn.textContent = 'Verify';
                
                if (data.valid) {
                    showVerifySuccess('Code valid! Redirecting...');
                    setTimeout(() => {
                        window.location.href = `/verify/${code}`;
                    }, 1000);
                } else {
                    showVerifyError('Invalid code. Please check and try again.');
                }
            })
            .catch(error => {
                console.error('Error checking code:', error);
                submitCodeBtn.disabled = false;
                submitCodeBtn.textContent = 'Verify';
                showVerifyError('Error checking code. Please try again.');
            });
    });
    
    // Handle Enter key in code input
    codeInput.addEventListener('keypress', (e) => {
        if (e.key === 'Enter') {
            submitCodeBtn.click();
        }
    });
    
    // Initialize
    initSocket();
});


===== requirements.txt =====

bidict==0.23.1
blinker==1.9.0
cffi==1.17.1
click==8.1.8
contourpy==1.3.1
cycler==0.12.1
dlib==19.24.6
dnspython==2.7.0
eventlet==0.39.0
filelock==3.18.0
Flask==3.1.0
Flask-SocketIO==5.5.1
fonttools==4.56.0
fsspec==2025.3.0
greenlet==3.1.1
h11==0.14.0
itsdangerous==2.2.0
Jinja2==3.1.5
kiwisolver==1.4.8
MarkupSafe==3.0.2
mpmath==1.3.0
networkx==3.4.2
numpy==2.2.3
opencv-python==4.11.0.86
packaging==24.2
pillow==11.1.0
pocketsphinx==5.0.4
PyAudio==0.2.14
pycparser==2.22
pyparsing==3.2.1
python-dateutil==2.9.0.post0
python-engineio==4.11.2
python-socketio==5.12.1
qrcode==8.0
setuptools==76.0.0
simple-websocket==1.1.0
six==1.17.0
sounddevice==0.5.1
SpeechRecognition==3.14.1
sympy==1.13.1
typing_extensions==4.12.2
Werkzeug==3.1.3
wsproto==1.2.0


===== docker-compose.yml =====

services:
  facedetection:
    build: .
    container_name: facedetection
    ports:
      - "8001:8080"  # external:internal, so you can reach it at http://serverip:8001
    environment:
      # if you want to override config port, do: PORT=8001, etc.
      - PORT=8080
    restart: unless-stopped



===== Dockerfile =====

FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    pkg-config \
    libx11-dev \
    libgtk-3-dev \
    libatlas-base-dev \
    libjpeg-dev \
    libpng-dev \
    libavformat-dev \
    libavcodec-dev \
    libavdevice-dev \
    libavfilter-dev \
    libswscale-dev \
    libv4l-dev \
    libopenblas-dev \
    liblapack-dev \
    libpq-dev \
    portaudio19-dev \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --upgrade pip setuptools wheel
RUN pip install --no-cache-dir -r requirements.txt

COPY . /app

EXPOSE 8080

CMD ["python", "web_app.py"]


