
===== blink_detector.py =====

"""Eye detection and blink analysis module."""

import cv2
import numpy as np
import time
import logging
import dlib
from typing import Tuple, Optional
from collections import deque

from config import Config

class BlinkDetector:
    """Handles eye detection and blink analysis using facial landmarks."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load eye detectors (as fallback if no dlib)
        self.eye_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        if self.eye_detector.empty():
            self.logger.warning("Failed to load eye detector cascade")
        
        # Load dlib face detector + shape predictor
        self.dlib_detector = dlib.get_frontal_face_detector()
        try:
            self.dlib_predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
            self.using_dlib = True
            self.logger.info("Using dlib for facial landmark detection")
        except Exception as e:
            self.logger.warning(f"Could not load dlib shape predictor: {e}")
            self.logger.warning("Falling back to Haar cascade for eye detection")
            self.using_dlib = False
        
        # Blink detection variables
        self.blink_threshold = config.BLINK_THRESHOLD
        self.min_blink_frames = config.MIN_BLINK_FRAMES
        self.blink_frames = 0
        self.blink_counter = 0
        self.blink_detected = False
        self.last_blink_time = time.time()
        
        # Track EAR
        self.ear_history = deque(maxlen=30)
        self.eye_state = "open"  # can be "open", "closing", "closed", "opening"
        self.eye_state_start = time.time()
    
    def calculate_ear(self, eye_points: np.ndarray) -> float:
        # Eye Aspect Ratio
        A = np.linalg.norm(eye_points[1] - eye_points[5])
        B = np.linalg.norm(eye_points[2] - eye_points[4])
        C = np.linalg.norm(eye_points[0] - eye_points[3])
        if C == 0:
            return 0
        return (A + B) / (2.0 * C)
    
    def detect_blinks_dlib(self, frame: np.ndarray,
                           face_rect: Tuple[int,int,int,int]) -> bool:
        """Detect blinks using dlib EAR. Draw lines on `frame` for debug."""
        if face_rect is None:
            return False
        
        x, y, w, h = face_rect
        rect = dlib.rectangle(x, y, x + w, y + h)
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        landmarks = self.dlib_predictor(gray, rect)
        
        left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])
        right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])
        
        left_ear = self.calculate_ear(left_eye)
        right_ear = self.calculate_ear(right_eye)
        avg_ear = (left_ear + right_ear) / 2.0
        self.ear_history.append(avg_ear)
        
        # Draw eye contours for debugging
        for eye in [left_eye, right_eye]:
            for i in range(len(eye)):
                pt1 = tuple(eye[i])
                pt2 = tuple(eye[(i+1) % 6])
                cv2.line(frame, pt1, pt2, (0,255,0), 1)
        
        self.logger.debug(f"EAR: {avg_ear:.2f} (Threshold: {self.blink_threshold:.2f})")
        
        current_time = time.time()
        blink_detected_now = False
        
        if avg_ear < self.blink_threshold:
            self.blink_frames += 1
            if self.eye_state == "open":
                self.eye_state = "closing"
                self.eye_state_start = current_time
            elif (self.eye_state == "closing"
                  and (current_time - self.eye_state_start) > 0.1):
                self.eye_state = "closed"
                self.eye_state_start = current_time
        else:
            if (self.eye_state == "closed"
                and self.blink_frames >= self.min_blink_frames
                and (current_time - self.last_blink_time) > self.config.MIN_BLINK_INTERVAL):
                self.blink_counter += 1
                self.blink_detected = True
                blink_detected_now = True
                self.last_blink_time = current_time
                self.logger.info(f"BLINK DETECTED! Counter: {self.blink_counter}")
            
            # revert to open or opening
            self.eye_state = "open" if self.eye_state != "closed" else "opening"
            self.eye_state_start = current_time
            self.blink_frames = 0
        
        # Display EAR + blink count in face ROI
        face_roi = frame[y : y + h, x : x + w]
        cv2.putText(face_roi, f"EAR: {avg_ear:.2f}", (10,40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        cv2.putText(face_roi, f"Blinks: {self.blink_counter}", (10,20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        
        return blink_detected_now
    
    def detect_blinks_haar(self, face_roi: np.ndarray,
                           frame: np.ndarray,
                           face_rect: Tuple[int,int,int,int]) -> bool:
        """Fallback blink detection with Haar. Extremely simplistic."""
        if face_roi.shape[0]<20 or face_roi.shape[1]<20:
            return False
        
        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        gray_face = cv2.equalizeHist(gray_face)
        
        eyes = self.eye_detector.detectMultiScale(gray_face, 1.1,3, minSize=(20,20))
        if len(eyes)<2:
            eyes = self.eye_detector.detectMultiScale(gray_face, 1.05,2, minSize=(15,15))
        
        x,y,w,h = face_rect
        blink_detected_now = False
        if len(eyes)==0:
            current_time = time.time()
            if (current_time - self.last_blink_time) > self.config.MIN_BLINK_INTERVAL:
                self.blink_counter += 1
                self.blink_detected = True
                blink_detected_now = True
                self.logger.debug(f"BLINK DETECTED! Counter: {self.blink_counter}")
                self.last_blink_time = current_time
        else:
            # draw eyes for debug
            for (ex,ey,ew,eh) in eyes:
                cv2.rectangle(frame, (x+ex,y+ey), (x+ex+ew, y+ey+eh), (0,255,0),1)
        
        # show blink count
        roi = frame[y : y+h, x : x+w]
        cv2.putText(roi, f"Blinks: {self.blink_counter}", (10,20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)
        return blink_detected_now
    
    def detect_blinks(self,
                      frame: np.ndarray,
                      face_rect: Tuple[int,int,int,int],
                      face_roi: np.ndarray) -> bool:
        """
        Decide which method to use: dlib or Haar.
        We draw debug lines right on `frame`.
        """
        if self.using_dlib:
            return self.detect_blinks_dlib(frame, face_rect)
        else:
            return self.detect_blinks_haar(face_roi, frame, face_rect)
    
    def reset(self) -> None:
        self.blink_counter = 0
        self.blink_detected = False
        self.blink_frames = 0
        self.eye_state = "open"

    def detect_blinks_with_debug(self, frame, face_rect, face_roi):
        """
        Detect blinks and draw detailed eye information on the debug frame.
        Similar to detect_blinks but with additional debug visualization.
        """
        if not self.using_dlib:
            return 0
        
        # Convert face_roi to grayscale for dlib
        gray_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        
        # Convert OpenCV face rect to dlib rect
        x, y, w, h = face_rect
        dlib_rect = dlib.rectangle(x, y, x + w, y + h)
        
        # Get facial landmarks
        landmarks = self.dlib_predictor(gray_roi, dlib_rect)
        
        # Calculate Eye Aspect Ratio (EAR)
        left_eye_coords = np.array([(landmarks.part(i).x, landmarks.part(i).y) 
                                   for i in range(36, 42)])
        right_eye_coords = np.array([(landmarks.part(i).x, landmarks.part(i).y) 
                                    for i in range(42, 48)])
        
        # Draw eye polygons
        cv2.polylines(frame, [left_eye_coords], True, (0, 255, 0), 1)
        cv2.polylines(frame, [right_eye_coords], True, (0, 255, 0), 1)
        
        # Calculate EAR
        left_ear = self.calculate_ear(left_eye_coords)
        right_ear = self.calculate_ear(right_eye_coords)
        ear = (left_ear + right_ear) / 2.0
        self.last_ear = ear
        
        # Draw EAR value for each eye
        left_center = np.mean(left_eye_coords, axis=0).astype(int)
        right_center = np.mean(right_eye_coords, axis=0).astype(int)
        
        cv2.putText(frame, f"L: {left_ear:.2f}", 
                    (left_center[0] - 20, left_center[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        cv2.putText(frame, f"R: {right_ear:.2f}", 
                    (right_center[0] - 20, right_center[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # Check for blink
        current_time = time.time()
        if ear < self.blink_threshold:
            self.frames_below_threshold += 1
        else:
            if (self.frames_below_threshold >= self.min_blink_frames and
                    current_time - self.last_blink_time > self.min_blink_interval):
                self.blink_counter += 1
                self.last_blink_time = current_time
                print(f"Blink detected! Count: {self.blink_counter}")
            self.frames_below_threshold = 0
        
        return self.blink_counter



===== challenge_manager.py =====

"""Challenge management module for liveness verification."""

import time
import random
import logging
from typing import Optional, Tuple

from config import Config

class ChallengeManager:
    """Manages challenge issuance and verification."""
    
    def __init__(self, config: Config, speech_recognizer=None, blink_detector=None):
        self.config = config
        self.speech_recognizer = speech_recognizer
        self.blink_detector = blink_detector
        self.logger = logging.getLogger(__name__)
        
        self.current_challenge = None
        self.challenge_completed = False
        self.challenge_start_time = None
        self.challenge_timeout = config.CHALLENGE_TIMEOUT
        self.available_challenges = config.CHALLENGES
        self.challenge_action_completed = False
        self.challenge_word_completed = False
        self.verification_result = None
        self.action_completion_time = None
        self.word_completion_time = None
    
    def issue_new_challenge(self) -> str:
        self.current_challenge = random.choice(self.available_challenges)
        self.challenge_start_time = time.time()
        self.challenge_completed = False
        self.challenge_action_completed = False
        self.challenge_word_completed = False
        self.verification_result = None
        self.action_completion_time = None
        self.word_completion_time = None
        
        if self.speech_recognizer:
            self.speech_recognizer.reset()
        if self.blink_detector:
            self.blink_detector.reset()
            self.logger.debug("Blink counter reset for new challenge")
        
        self.logger.info(f"New challenge issued: {self.current_challenge}")
        return self.current_challenge
    
    def verify_challenge(self, head_pose: str, blink_counter: int, last_speech: str) -> bool:
        if self.current_challenge is None:
            return False
        
        self.logger.debug(f"Verifying - Head: {head_pose}, Blinks: {blink_counter}, Speech: '{last_speech}'")
        
        elapsed = time.time() - self.challenge_start_time
        if elapsed > self.challenge_timeout:
            self.verification_result = "FAIL"
            self.current_challenge = None
            if self.speech_recognizer:
                self.speech_recognizer.reset()
            self.logger.info("Challenge timed out")
            return True
        
        c = self.current_challenge.lower()
        
        # Action check
        if not self.challenge_action_completed:
            if "turn left" in c and head_pose=="left":
                self.challenge_action_completed=True
                self.action_completion_time=time.time()
                self.logger.debug("LEFT ACTION COMPLETED!")
            elif "turn right" in c and head_pose=="right":
                self.challenge_action_completed=True
                self.action_completion_time=time.time()
                self.logger.debug("RIGHT ACTION COMPLETED!")
            elif "look up" in c and head_pose=="up":
                self.challenge_action_completed=True
                self.action_completion_time=time.time()
                self.logger.debug("UP ACTION COMPLETED!")
            elif "look down" in c and head_pose=="down":
                self.challenge_action_completed=True
                self.action_completion_time=time.time()
                self.logger.debug("DOWN ACTION COMPLETED!")
            elif "blink twice" in c and blink_counter>=2:
                self.challenge_action_completed=True
                self.action_completion_time=time.time()
                self.logger.debug(f"BLINK ACTION COMPLETED! Counter: {blink_counter}")
            elif "nod your head" in c:
                # if you detect nod
                pass
            elif "shake your head" in c:
                # if you detect shake
                pass
        
        # Word check
        if not self.challenge_word_completed:
            if "say blue" in c and "blue" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("BLUE WORD COMPLETED!")
            elif "say red" in c and "red" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("RED WORD COMPLETED!")
            elif "say sky" in c and "sky" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("SKY WORD COMPLETED!")
            elif "say ground" in c and "ground" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("GROUND WORD COMPLETED!")
            elif "say hello" in c and "hello" in last_speech:
                self.challenge_word_completed=True
                self.word_completion_time=time.time()
                self.logger.debug("HELLO WORD COMPLETED!")
        
        # Check concurrency
        if self.challenge_action_completed and self.challenge_word_completed:
            diff = abs((self.action_completion_time or 0) - (self.word_completion_time or 0))
            self.logger.debug(f"Time diff between action & speech: {diff:.2f}s (max {self.config.ACTION_SPEECH_WINDOW:.2f}s)")
            if diff <= self.config.ACTION_SPEECH_WINDOW:
                self.challenge_completed = True
                self.verification_result = "PASS"
                self.current_challenge = None
                if self.speech_recognizer:
                    self.speech_recognizer.reset()
                self.logger.info("Challenge PASSED!")
                return True
            else:
                self.logger.debug(f"Action & speech not concurrent (diff: {diff:.2f}s)")
        
        return False
    
    def get_challenge_status(self) -> Tuple[Optional[str],bool,bool,Optional[str]]:
        return (
            self.current_challenge,
            self.challenge_action_completed,
            self.challenge_word_completed,
            self.verification_result
        )
    
    def get_challenge_time_remaining(self) -> float:
        if self.current_challenge is None or self.challenge_start_time is None:
            return 0
        elapsed = time.time() - self.challenge_start_time
        return max(0, self.challenge_timeout - elapsed)
    
    def update(self, blink_counter: int, head_pose: str, last_speech: str) -> None:
        """
        Update the challenge manager with the latest detection results.
        This method is called every frame to check challenge status.
        
        Args:
            blink_counter: Number of blinks detected
            head_pose: Current head pose ("left", "right", "up", "down", etc.)
            last_speech: Last recognized speech
        """
        # This is a simple passthrough to verify_challenge
        if self.current_challenge:
            self.verify_challenge(head_pose, blink_counter, last_speech)



===== config.py =====

"""Configuration settings for the liveness detection system."""

class Config:
    # Debug mode
    DEBUG = True
    
    # Show debug frame with eye tracking polygons, EAR values, etc.
    SHOW_DEBUG_FRAME = True
    
    # Camera settings
    CAMERA_WIDTH = 640
    CAMERA_HEIGHT = 480
    
    # Face detection parameters
    FACE_CONFIDENCE_THRESHOLD = 0.9
    FACE_NMS_THRESHOLD = 0.3
    
    # Head pose thresholds (normalized)
    HEAD_POSE_THRESHOLD_X = 0.06  # 8% of half frame width
    HEAD_POSE_THRESHOLD_Y_UP = 0.08  # 8% for looking up
    HEAD_POSE_THRESHOLD_Y_DOWN = 0.10  # 10% for looking down
    
    # History lengths for tracking
    LANDMARK_HISTORY_MAX = 30  # Maximum frames to keep in landmark history
    FACE_POSITION_HISTORY_LENGTH = 30  # Length of face position/angle history
    
    # Blink detection parameters
    BLINK_THRESHOLD = 0.25  # EAR threshold for blink detection
    MIN_BLINK_FRAMES = 1  # Minimum consecutive frames below threshold to count as blink
    MIN_BLINK_INTERVAL = 0.1  # Minimum time between blinks (seconds)
    
    # Challenge parameters
    CHALLENGE_TIMEOUT = 10  # seconds
    ACTION_SPEECH_WINDOW = 5.0  # seconds allowed between action and speech
    
    # Speech recognition parameters
    SPEECH_TIMEOUT = 5  # seconds
    SPEECH_PHRASE_LIMIT = 2  # seconds
    SPEECH_SAMPLING_RATE = 16000
    SPEECH_BUFFER_SIZE = 1024
    SPEECH_KEYWORDS = [
        "blue /1e-3/",
        "red /1e-3/",
        "sky /1e-3/",
        "ground /1e-3/",
        "hello /1e-3/",
        "noise /1e-1/"
    ]
    
    # Liveness scoring
    MIN_CONSECUTIVE_LIVE_FRAMES = 5
    MIN_CONSECUTIVE_FAKE_FRAMES = 5
    
    # Logging
    LOGGING_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Available challenges
    CHALLENGES = [
        "Turn left and say blue", 
        "Turn right and say red", 
        "Look up and say sky", 
        "Look down and say ground", 
        "Blink twice and say hello"
    ]


===== face_detector.py =====

"""Face detection and head pose estimation module."""

import cv2
import numpy as np
from collections import deque
import logging
from typing import Tuple, Optional

from config import Config

class FaceDetector:
    """Handles face detection and head pose estimation."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load cascade
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.logger.info(f"Attempting to load cascade from: {cascade_path}")
        self.face_detector = cv2.CascadeClassifier(cascade_path)
        if self.face_detector.empty():
            self.logger.error("Failed to load face detector cascade")
            raise ValueError("Failed to load face detector cascade")
        self.logger.info("Face detector cascade loaded successfully")
        
        self.face_positions = deque(maxlen=30)
        self.face_angles = deque(maxlen=30)
        self.head_pose = "center"
        self.movement_detected = False
    
    def detect_face(self, frame: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[Tuple[int,int,int,int]]]:
        self.logger.debug(f"Detecting face in frame with shape: {frame.shape if frame is not None else 'None'}")
        if frame is None or frame.size == 0:
            self.logger.error("Received empty or None frame")
            return None, None
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_detector.detectMultiScale(gray, 1.1, 5)
        
        if len(faces) == 0:
            faces = self.face_detector.detectMultiScale(gray, 1.05, 3, minSize=(30, 30))
        if len(faces) == 0:
            faces = self.face_detector.detectMultiScale(gray, 1.03, 2, minSize=(20, 20))
        
        # Fallback logic: If no face is detected, use the last known position
        if len(faces) == 0 and len(self.face_positions) > 0:
            last_x, last_y = self.face_positions[-1]
            est_size = 150  # Estimated size for fallback
            x = int(last_x - est_size // 2)
            y = int(last_y - est_size // 2)
            w = est_size
            h = est_size
            self.logger.debug("Using estimated face position fallback")
            
            x = max(0, x)
            y = max(0, y)
            w = min(w, frame.shape[1] - x)
            h = min(h, frame.shape[0] - y)
            
            if w > 0 and h > 0:
                face_roi = frame[y:y+h, x:x+w]
                return face_roi, (x, y, w, h)
            else:
                self.logger.debug("Fallback face ROI invalid")
                return None, None
        
        if len(faces) == 0:
            self.logger.debug("No face detected in frame")
            return None, None
        
        face_rect = max(faces, key=lambda rect: rect[2] * rect[3])
        x, y, w, h = face_rect
        x = max(0, x)
        y = max(0, y)
        w = min(w, frame.shape[1] - x)
        h = min(h, frame.shape[0] - y)
        if w <= 0 or h <= 0:
            self.logger.debug("Detected face ROI invalid")
            return None, None
        
        face_roi = frame[y:y+h, x:x+w]
        self.logger.debug(f"Face detected at: ({x}, {y}, {w}, {h})")
        return face_roi, (x, y, w, h)
    
    def detect_movement(self, face_rect: Tuple[int,int,int,int]) -> bool:
        if face_rect is None:
            return False
        x,y,w,h = face_rect
        cx = x + w/2
        cy = y + h/2
        
        self.face_positions.append((cx,cy))
        if len(self.face_positions)<2:
            return False
        
        positions = list(self.face_positions)
        movement=0
        for i in range(1,len(positions)):
            dx = positions[i][0]-positions[i-1][0]
            dy = positions[i][1]-positions[i-1][1]
            movement += np.sqrt(dx*dx + dy*dy)
        
        avg_movement = movement / (len(positions)-1)
        self.movement_detected = avg_movement>2.0
        return self.movement_detected
    
    def detect_head_pose(self, frame: np.ndarray,
                         face_rect: Tuple[int,int,int,int]) -> str:
        if face_rect is None:
            return self.head_pose
        
        x,y,w,h = face_rect
        face_cx = x + w/2
        face_cy = y + h/2
        frame_cx = frame.shape[1]/2
        frame_cy = frame.shape[0]/2
        
        x_offset = face_cx - frame_cx
        y_offset = face_cy - frame_cy
        
        x_offset_norm = x_offset/(frame.shape[1]/2)
        y_offset_norm = y_offset/(frame.shape[0]/2)
        
        self.face_angles.append((x_offset_norm,y_offset_norm))
        
        if len(self.face_angles)>=5:
            angles_list = list(self.face_angles)
            avg_x = sum(a[0] for a in angles_list)/len(angles_list)
            avg_y = sum(a[1] for a in angles_list)/len(angles_list)
            
            x_thr = self.config.HEAD_POSE_THRESHOLD_X
            y_thr_up = self.config.HEAD_POSE_THRESHOLD_Y_UP
            y_thr_down = self.config.HEAD_POSE_THRESHOLD_Y_DOWN
            
            old_pose = self.head_pose
            if avg_x < -x_thr:
                self.head_pose="right"
            elif avg_x > x_thr:
                self.head_pose="left"
            elif avg_y < -y_thr_up:
                self.head_pose="up"
            elif avg_y > y_thr_down:
                self.head_pose="down"
            else:
                self.head_pose="center"
            
            if old_pose != self.head_pose:
                self.logger.debug(f"{self.head_pose.upper()} detected!")
            
            # draw line for debug
            center_x = int(frame.shape[1]/2)
            center_y = int(frame.shape[0]/2)
            dir_x = int(center_x + avg_x*100)
            dir_y = int(center_y + avg_y*100)
            cv2.line(frame, (center_x,center_y), (dir_x,dir_y), (0,255,255),2)
        
        return self.head_pose
    
    def draw_face_info(self, frame: np.ndarray,
                       face_rect: Tuple[int,int,int,int],
                       status: str,
                       score: float) -> None:
        if face_rect is None:
            return
        x,y,w,h = face_rect
        
        color = (0,0,255)  # default red
        if status=="Live Person":
            color = (0,255,0)
        elif status=="Analyzing...":
            color = (0,165,255)
        
        cv2.rectangle(frame, (x,y), (x+w, y+h), color,2)
        
        cv2.putText(frame, f"Status: {status}", (x,y-40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color,2)
        cv2.putText(frame, f"Score: {score:.2f}", (x,y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color,2)
        
        cv2.putText(frame, f"Head: {self.head_pose}",
                    (10, frame.shape[0]-50),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                    (255,255,255),2)


===== liveness_detector.py =====

"""Main liveness detection module integrating all components."""

import cv2
import numpy as np
import time
import logging
from typing import Tuple, Optional
import dlib

from config import Config
from face_detector import FaceDetector
from blink_detector import BlinkDetector
from speech_recognizer import SpeechRecognizer
from challenge_manager import ChallengeManager
from action_detector import ActionDetector

class LivenessDetector:
    """Main class for liveness detection integrating all components."""
    
    def __init__(self, config: Config):
        """Initialize the liveness detector with configuration."""
        self.config = config
        
        # Configure logging
        logging_level = logging.DEBUG if config.DEBUG else logging.INFO
        logging.basicConfig(
            level=logging_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # If not in debug mode, set logging level to WARNING to suppress INFO logs
        if not config.DEBUG:
            for handler in logging.root.handlers:
                handler.setLevel(logging.WARNING)
        
        # Initialize components
        self.face_detector = FaceDetector(config)
        self.blink_detector = BlinkDetector(config)
        self.action_detector = ActionDetector(config)
        self.speech_recognizer = SpeechRecognizer(config)
        
        # Pass speech_recognizer and blink_detector to challenge_manager for reset coordination
        self.challenge_manager = ChallengeManager(
            config, 
            speech_recognizer=self.speech_recognizer,
            blink_detector=self.blink_detector
        )
        
        # Status variables
        self.consecutive_live_frames = 0
        self.consecutive_fake_frames = 0
        self.status = "Analyzing..."
        self.liveness_score = 0.0
        
        # Start with a challenge immediately
        self.challenge_manager.issue_new_challenge()
        self.speech_recognizer.start_listening()
        
        # Set target word from challenge
        challenge_text, _, _, _ = self.challenge_manager.get_challenge_status()
        if challenge_text:
            target_word = challenge_text.split()[-1]
            self.speech_recognizer.set_target_word(target_word)
    
    def detect_liveness(self, frame: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        Process a frame for liveness detection (the old function).
        
        Returns: (processed_frame, exit_flag).
        This is still used in some older code, apparently.
        """
        # Make a copy of the frame for display
        display_frame = frame.copy()
        
        # Detect face
        face_detection_result = self.face_detector.detect_face(frame)
        
        # Handle the case where detect_face returns either (face_roi, face_rect) or just face_rect
        if isinstance(face_detection_result, tuple) and len(face_detection_result) == 2 and isinstance(face_detection_result[1], tuple):
            # It returned (face_roi, face_rect)
            face_roi, face_rect = face_detection_result
        else:
            # It returned just face_rect
            face_rect = face_detection_result
            # Extract face ROI manually if face_rect exists
            if face_rect is not None:
                x, y, w, h = face_rect
                face_roi = frame[y:y+h, x:x+w]
            else:
                face_roi = None
        
        # If no face detected, show a message
        if face_roi is None:
            cv2.putText(display_frame, "No face detected", (50, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            return display_frame, False
        
        # Detect movement
        self.face_detector.detect_movement(face_rect)
        
        # Detect head pose using action detector
        head_pose = self.action_detector.detect_head_pose(display_frame, face_rect)
        
        # Detect blinks
        self.blink_detector.detect_blinks(frame, face_rect, face_roi)
        
        # Get last speech
        last_speech = self.speech_recognizer.get_last_speech()
        
        # Verify current challenge or issue new one
        challenge_text, action_completed, word_completed, verification_result = \
            self.challenge_manager.get_challenge_status()
        
        if challenge_text is not None:
            # Verify challenge
            self.challenge_manager.verify_challenge(
                head_pose, self.blink_detector.blink_counter, last_speech
            )
            
            # Display challenge status
            cv2.putText(display_frame, f"Challenge: {challenge_text}", (10, 30), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            time_remaining = self.challenge_manager.get_challenge_time_remaining()
            cv2.putText(display_frame, f"Time: {time_remaining:.1f}s", (10, 60), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Display action and word status
            action_status = "✓" if action_completed else "✗"
            word_status = "✓" if word_completed else "✗"
            cv2.putText(display_frame, f"Action: {action_status}", (10, 90), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(display_frame, f"Word: {word_status}", (10, 120), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Check verification result
            if verification_result == "PASS":
                cv2.putText(display_frame, "VERIFICATION PASSED", (50, 200), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                return display_frame, True
            elif verification_result == "FAIL":
                cv2.putText(display_frame, "VERIFICATION FAILED", (50, 200), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                return display_frame, True
        else:
            # Issue a new challenge if none is active
            self.challenge_manager.issue_new_challenge()
        
        # Draw face information
        self.face_detector.draw_face_info(display_frame, face_rect, self.status, self.liveness_score)
        
        # Display speech recognition status
        cv2.putText(display_frame, f"Speech: {last_speech}", (10, display_frame.shape[0]-20), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return display_frame, False

    def reset(self) -> None:
        """Reset all components."""
        self.blink_detector.reset()
        self.speech_recognizer.reset()
        self.challenge_manager.issue_new_challenge()
        self.consecutive_live_frames = 0
        self.consecutive_fake_frames = 0
        self.status = "Analyzing..."
        self.liveness_score = 0.0
    
    def start_challenge(self):
        """Start a new challenge."""
        self.challenge_manager.start_new_challenge()
        challenge_text, _, _, _ = self.challenge_manager.get_challenge_status()
        if challenge_text:
            target_word = challenge_text.split()[-1]
            self.speech_recognizer.set_target_word(target_word)
    
    def process_frame(self, frame):
        """
        Process a frame for liveness detection.
        
        Returns: Dictionary with processed frame data.
        """
        if frame is None or frame.size == 0:
            print("Error: Frame is None or empty in process_frame")
            return {
                'display_frame': None,
                'debug_frame': None,
                'verification_result': 'PENDING',
                'exit_flag': False,
                'challenge_text': None,
                'action_completed': False,
                'word_completed': False,
                'time_remaining': 0
            }
        
        # 1) Make two copies
        display_frame = frame.copy()
        debug_frame = frame.copy()  # Always create debug frame
        
        # 2) Face detection
        face_roi, face_rect = self.face_detector.detect_face(display_frame)
        
        if face_roi is None:
            cv2.putText(display_frame, "No face detected", (30, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            cv2.putText(debug_frame, "No face detected", (30, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
        
            return {
                'display_frame': display_frame,
                'debug_frame': debug_frame,
                'verification_result': 'PENDING',
                'exit_flag': False,
                'challenge_text': None,
                'action_completed': False,
                'word_completed': False,
                'time_remaining': 0
            }
        
        # 3) Blink detection - use regular detect_blinks to avoid breaking functionality
        blink_count = self.blink_detector.detect_blinks(frame, face_rect, face_roi)
        
        # Draw eye polygons on debug frame if needed
        if self.config.SHOW_DEBUG_FRAME:
            # Get facial landmarks for visualization
            gray_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
            x, y, w, h = face_rect
            dlib_rect = dlib.rectangle(x, y, x + w, y + h)
            landmarks = self.blink_detector.dlib_predictor(gray_roi, dlib_rect)
            
            # Draw eye contours
            left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])
            right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])
            
            cv2.polylines(debug_frame, [left_eye], True, (0, 255, 0), 1)
            cv2.polylines(debug_frame, [right_eye], True, (0, 255, 0), 1)
            
            # Calculate and display EAR values
            left_ear = self.blink_detector.calculate_ear(left_eye)
            right_ear = self.blink_detector.calculate_ear(right_eye)
            
            left_center = np.mean(left_eye, axis=0).astype(int)
            right_center = np.mean(right_eye, axis=0).astype(int)
            
            cv2.putText(debug_frame, f"L: {left_ear:.2f}", 
                        (left_center[0] - 20, left_center[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            cv2.putText(debug_frame, f"R: {right_ear:.2f}", 
                        (right_center[0] - 20, right_center[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # 4) Action detection
        action_detected = self.action_detector.detect_action(face_roi, face_rect)
        
        # 5) Speech recognition
        last_speech = self.speech_recognizer.get_last_speech()
        
        # 6) Update challenge manager
        self.challenge_manager.update(blink_count, action_detected, last_speech)
        
        # 7) Get challenge status
        challenge_text, action_completed, word_completed, verification_result = \
            self.challenge_manager.get_challenge_status()
        
        time_left = self.challenge_manager.get_challenge_time_remaining()
        
        # 8) Prepare result
        final_result = 'PENDING'
        exit_flag = False
        
        if verification_result != "PENDING":
            # Add debug info to debug frame
            cv2.putText(debug_frame, f"Action: {action_detected}", (10, 120),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Speech: {last_speech}", (10, 150),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(debug_frame, f"Blinks: {self.blink_detector.blink_counter}", (10, 180),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            if verification_result == "PASS":
                cv2.putText(debug_frame, "VERIFICATION PASSED", (50, 220),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)
                final_result = 'PASS'
                exit_flag = True
            elif verification_result == "FAIL":
                cv2.putText(debug_frame, "VERIFICATION FAILED", (50, 220),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                final_result = 'FAIL'
                exit_flag = True
        else:
            # If no challenge is active, issue a new one
            if not challenge_text:
                self.challenge_manager.issue_new_challenge()
                challenge_text, action_completed, word_completed, verification_result = \
                    self.challenge_manager.get_challenge_status()
                time_left = self.challenge_manager.get_challenge_time_remaining()
                print(f"New challenge issued: {challenge_text}, Time left: {time_left:.1f}s")
        
        # Minimal info on display_frame
        self.face_detector.draw_face_info(display_frame, face_rect, self.status, self.liveness_score)
        cv2.putText(display_frame, f"Speech: {last_speech}",
                    (10, display_frame.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Always add debug info to debug frame
        cv2.putText(debug_frame, f"Challenge: {challenge_text}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Action completed: {action_completed}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Word completed: {word_completed}", (10, 90),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Time left: {time_left:.1f}s", (10, 120),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Blinks: {self.blink_detector.blink_counter}", (10, 150),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Update liveness score based on challenge progress
        if action_completed:
            self.liveness_score += 0.5
        if word_completed:
            self.liveness_score += 0.5
        # Cap score at 1.0
        self.liveness_score = min(1.0, self.liveness_score)
        
        return {
            'display_frame': display_frame,
            'debug_frame': debug_frame,
            'verification_result': final_result,
            'exit_flag': exit_flag,
            'challenge_text': challenge_text,
            'action_completed': action_completed,
            'word_completed': word_completed,
            'time_remaining': time_left
        }


===== main.py =====

"""Main application for liveness detection."""

import cv2
import time
import argparse
import logging
from typing import Optional

from config import Config
from liveness_detector import LivenessDetector

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Liveness Detection System")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--camera", type=int, default=0, help="Camera index")
    return parser.parse_args()

def main():
    """Main application entry point."""
    # Parse command line arguments
    args = parse_args()
    
    # Create configuration
    config = Config()
    config.DEBUG = args.debug
    
    # Configure logging
    logging_level = logging.DEBUG if config.DEBUG else logging.INFO
    logging.basicConfig(
        level=logging_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)
    
    # Initialize camera
    logger.info(f"Opening camera {args.camera}")
    cap = cv2.VideoCapture(args.camera)
    
    if not cap.isOpened():
        logger.error("Error: Could not open camera")
        return
    
    # Set camera properties
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # Initialize liveness detector
    detector = LivenessDetector(config)
    
    # Main loop
    while True:
        # Read frame
        ret, frame = cap.read()
        
        if not ret:
            logger.error("Error: Could not read frame")
            break
        
        # Process frame
        display_frame, exit_flag = detector.detect_liveness(frame)
        
        # Display frame
        cv2.imshow("Liveness Detection", display_frame)
        
        # Check for exit
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or exit_flag:
            break
        elif key == ord('r'):
            # Reset detector
            detector.reset()
    
    # Release resources
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()




===== speech_recognizer.py =====

"""Speech recognition module for challenge verification using PocketSphinx."""

import speech_recognition as sr
import threading
import time
import logging
import tempfile
from config import Config
from pocketsphinx import LiveSpeech

class SpeechRecognizer:
    """Handles real-time speech recognition for challenge verification using PocketSphinx."""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        self.last_speech = ""
        self.speech_lock = threading.Lock()
        self.last_speech_time = 0
        self.running = False
        self.speech_thread = None
        self.speech_ready = False
        self.target_word = ""  # default target word is empty
        
        try:
            self.logger.info("Initializing PocketSphinx...")
            keywords = [
                "blue /1e-3/",
                "red /1e-3/",
                "sky /1e-3/",
                "ground /1e-3/",
                "hello /1e-3/",
                "noise /1e-1/"
            ]
            self.keyword_file = tempfile.NamedTemporaryFile(mode='w', delete=False).name
            with open(self.keyword_file, "w") as f:
                for kw in keywords:
                    f.write(kw + "\n")
            
            self.logger.info(f"Keyword file: {self.keyword_file}")
            
            self.microphone = sr.Microphone()
            with self.microphone as source:
                self.logger.info("Calibrating microphone for ambient noise (0.5s)...")
                sr.Recognizer().adjust_for_ambient_noise(source, duration=0.5)
            
            self.speech = LiveSpeech(
                verbose=False,
                sampling_rate=config.SPEECH_SAMPLING_RATE,
                buffer_size=config.SPEECH_BUFFER_SIZE,
                no_search=False,
                full_utt=False,
                kws=self.keyword_file
            )
            self.speech_ready = True
            self.logger.info("PocketSphinx ready.")
        except ImportError:
            self.logger.error("Could not import pocketsphinx. Please install via pip")
            self.speech_ready = False
        except Exception as e:
            self.logger.error(f"Error initializing pocketsphinx: {e}")
            self.speech_ready = False

    def set_target_word(self, word: str) -> None:
        """Sets the target word for the current challenge."""
        self.target_word = word.lower().strip()
        self.logger.info(f"Target word set to: {self.target_word}")
    
    def listen_for_speech(self) -> None:
        if not self.speech_ready:
            self.logger.warning("Speech recognition not available")
            return
        
        self.logger.info("Speech recognition thread started")
        self.running = True
        last_detected_word = None
        last_time = 0
        
        try:
            for phrase in self.speech:
                if not self.running:
                    break
                text = str(phrase).lower()
                now = time.time()
                
                # If a target word is set, check if it appears in the phrase.
                if self.target_word and self.target_word in text:
                    with self.speech_lock:
                        self.last_speech = self.target_word
                        self.last_speech_time = now
                    self.logger.info(f"Target word recognized: {self.target_word}")
                    continue

                # Otherwise, process the recognized keywords.
                possible_keywords = ["blue", "red", "sky", "ground", "hello", "noise"]
                first_word = None
                for k in possible_keywords:
                    if k in text:
                        first_word = k
                        break
                if first_word:
                    if first_word == "noise":
                        with self.speech_lock:
                            self.last_speech = ""
                            self.last_speech_time = now
                        last_detected_word = first_word
                        last_time = now
                        self.logger.debug("Detected 'noise' => ignoring")
                    else:
                        if (first_word != last_detected_word) or ((now - last_time) > 1.0):
                            with self.speech_lock:
                                self.last_speech = first_word
                                self.last_speech_time = now
                            self.logger.info(f"Recognized: {first_word} (full: '{text}')")
                            last_detected_word = first_word
                            last_time = now
                else:
                    self.logger.debug(f"No recognized keyword in '{text}'")
        except Exception as e:
            self.logger.error(f"Error in speech recognition: {e}")
        finally:
            self.running = False
            self.logger.info("Speech recognition thread ended")
    
    def start_listening(self) -> None:
        if not self.speech_ready:
            self.logger.warning("Speech not available")
            return
        if not self.running and (self.speech_thread is None or not self.speech_thread.is_alive()):
            self.speech_thread = threading.Thread(target=self.listen_for_speech, daemon=True)
            self.speech_thread.start()
    
    def stop(self) -> None:
        self.running = False
        if self.speech_thread and self.speech_thread.is_alive():
            self.logger.info("Stopping speech thread...")
            self.speech_thread.join(timeout=1.0)
    
    def get_last_speech(self) -> str:
        with self.speech_lock:
            return self.last_speech
    
    def get_last_speech_time(self) -> float:
        with self.speech_lock:
            return self.last_speech_time
    
    def reset(self) -> None:
        with self.speech_lock:
            self.last_speech = ""
            self.last_speech_time = 0



===== web_app.py =====

"""Web application for liveness detection."""

import os
import cv2
import base64
import numpy as np
import logging
import random
import string
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit, join_room, leave_room
import threading
import time
from typing import Dict, Any

from config import Config
from liveness_detector import LivenessDetector

# Initialize Flask app
app = Flask(__name__, 
            static_folder='static',
            template_folder='templates')
app.config['SECRET_KEY'] = 'liveness-detection-secret'
socketio = SocketIO(app, cors_allowed_origins="*")

# Create configuration
config = Config()

# Configure logging
logging_level = logging.DEBUG if config.DEBUG else logging.INFO
logging.basicConfig(
    level=logging_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Store active sessions
active_sessions: Dict[str, Dict[str, Any]] = {}

# Store verification codes
verification_codes: Dict[str, Dict[str, Any]] = {}

# Add this at the top of the file
last_log_time = {}

@app.route('/')
def index():
    """Render the main landing page."""
    return render_template('index.html')

@app.route('/verify/<code>')
def verify(code):
    """Render the verification page."""
    logger.debug(f"Verify route called with code: {code}")
    logger.debug(f"Current verification codes: {list(verification_codes.keys())}")
    
    if code not in verification_codes:
        return render_template('error.html', 
                               message="Invalid verification code", 
                               redirect_url="/")
    
    return render_template('verify.html', session_code=code)

@app.route('/check_code/<code>')
def check_code(code):
    """Check if a verification code is valid."""
    logger.debug(f"Check code route called with code: {code}")
    logger.debug(f"Current verification codes: {list(verification_codes.keys())}")
    
    is_valid = code in verification_codes
    return jsonify({'valid': is_valid})

@socketio.on('connect')
def handle_connect():
    """Handle client connection."""
    session_id = request.sid
    logger.info(f"Client connected: {session_id}")

@socketio.on('disconnect')
def handle_disconnect():
    """Handle client disconnection."""
    session_id = request.sid
    logger.info(f"Client disconnected: {session_id}")
    
    if session_id in active_sessions:
        if active_sessions[session_id]['detector'] is not None:
            active_sessions[session_id]['detector'].speech_recognizer.stop()
        del active_sessions[session_id]

@socketio.on('frame')
def handle_frame(data):
    """Process a frame from the client (older approach)."""
    session_id = request.sid
    
    if session_id not in active_sessions:
        logger.warning(f"Received frame from unknown session: {session_id}")
        return
    
    if active_sessions[session_id].get('attempts', 0) >= 3:
        emit('max_attempts_reached')
        return
    
    active_sessions[session_id]['last_activity'] = time.time()
    
    try:
        image_data = data['image'].split(',')[1]
        image_bytes = base64.b64decode(image_data)
        image_array = np.frombuffer(image_bytes, np.uint8)
        frame = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        
        detector = active_sessions[session_id]['detector']
        display_frame, exit_flag = detector.detect_liveness(frame)
        
        challenge_text, action_completed, word_completed, verification_result = \
            detector.challenge_manager.get_challenge_status()
        
        _, buffer = cv2.imencode('.jpg', display_frame)
        encoded_frame = base64.b64encode(buffer).decode('utf-8')
        
        emit('processed_frame', {
            'image': f'data:image/jpeg;base64,{encoded_frame}',
            'challenge': challenge_text,
            'action_completed': action_completed,
            'word_completed': word_completed,
            'time_remaining': detector.challenge_manager.get_challenge_time_remaining(),
            'verification_result': verification_result,
            'exit_flag': exit_flag
        })
        
        if exit_flag:
            active_sessions[session_id]['attempts'] = active_sessions[session_id].get('attempts', 0) + 1
            
    except Exception as e:
        logger.error(f"Error processing frame: {e}")
        emit('error', {'message': str(e)})

@socketio.on('reset')
def handle_reset(data):
    """Reset the verification process for a session."""
    session_id = request.sid
    code = data.get('code')
    
    if session_id not in active_sessions:
        logger.warning(f"Reset request from unknown session: {session_id}")
        return
    
    try:
        # Reset the detector's challenge manager
        detector = active_sessions[session_id]['detector']
        detector.challenge_manager.reset()
        
        # Force issue a new challenge
        detector.challenge_manager.issue_new_challenge()
        
        # Reset blink counter
        detector.blink_detector.blink_counter = 0
        
        # Get the new challenge and set target word
        challenge_text, _, _, _ = detector.challenge_manager.get_challenge_status()
        if challenge_text:
            target_word = challenge_text.split()[-1]
            detector.speech_recognizer.set_target_word(target_word)
            logger.info(f"New challenge issued after reset: {challenge_text}, Target word: {target_word}")
        
        logger.info(f"Reset verification for session {session_id}")
        
        # Emit confirmation
        emit('reset_confirmed')
        
    except Exception as e:
        logger.error(f"Error resetting verification: {e}", exc_info=True)
        emit('error', {'message': str(e)})

@socketio.on('get_debug_status')
def handle_get_debug_status():
    """Send the current debug status to the client."""
    emit('debug_status', {
        'debug': config.DEBUG,
        'showDebugFrame': config.SHOW_DEBUG_FRAME
    })

@socketio.on('generate_code')
def handle_generate_code():
    """Generate a verification code."""
    session_id = request.sid
    logger.info(f"Generate code request from session {session_id}")
    
    code = ''.join(random.choices(string.digits, k=6))
    verification_codes[code] = {
        'requester_id': session_id,
        'created_at': time.time(),
        'status': 'pending'
    }
    
    logger.info(f"Emitting verification code {code} to session {session_id}")
    emit('verification_code', {'code': code})
    
    logger.debug(f"Active verification codes: {verification_codes}")
    
    def expire_code():
        time.sleep(600)
        if code in verification_codes and verification_codes[code]['status'] == 'pending':
            del verification_codes[code]
            logger.info(f"Expired verification code {code}")
    
    expiration_thread = threading.Thread(target=expire_code)
    expiration_thread.daemon = True
    expiration_thread.start()

def cleanup_inactive_sessions():
    """Clean up inactive sessions periodically."""
    while True:
        current_time = time.time()
        inactive_sessions = []
        
        for session_id, session_data in active_sessions.items():
            if current_time - session_data['last_activity'] > 300:
                inactive_sessions.append(session_id)
        
        for session_id in inactive_sessions:
            logger.info(f"Cleaning up inactive session: {session_id}")
            if active_sessions[session_id]['detector'] is not None:
                active_sessions[session_id]['detector'].speech_recognizer.stop()
            del active_sessions[session_id]
        
        time.sleep(60)

@socketio.on('join_verification')
def handle_join_verification(data):
    """Handle client joining a verification session."""
    session_id = request.sid
    code = data.get('code')
    
    logger.info(f"Client {session_id} joining verification session with code: {code}")
    logger.debug(f"Current verification codes: {list(verification_codes.keys())}")
    
    if not code or code not in verification_codes:
        emit('session_error', {'message': 'Invalid verification code'})
        return
    
    if verification_codes[code]['status'] == 'in-progress':
        emit('session_error', {'message': 'This verification session is already in progress'})
        return
    
    verification_codes[code]['status'] = 'in-progress'
    verification_codes[code]['verifier_id'] = session_id
    
    active_sessions[session_id] = {
        'code': code,
        'detector': None,
        'last_activity': time.time(),
        'attempts': 0
    }
    
    detector = LivenessDetector(config)
    active_sessions[session_id]['detector'] = detector
    
    join_room(code)
    requester_id = verification_codes[code]['requester_id']
    emit('verification_started', {'code': code}, room=requester_id)
    
    logger.info(f"Client {session_id} joined verification session {code}")
    
    challenge_text, _, _, _ = detector.challenge_manager.get_challenge_status()
    emit('challenge', {'text': challenge_text})

@socketio.on('process_frame')
def handle_process_frame(data):
    """Process a frame from the client."""
    global last_log_time
    session_id = request.sid
    code = data.get('code')
    
    # Only log once per second per session
    current_time = time.time()
    if config.DEBUG and (session_id not in last_log_time or current_time - last_log_time.get(session_id, 0) >= 1.0):
        logger.debug(f"Processing frame for session {session_id}, code {code}")
        last_log_time[session_id] = current_time
    
    if session_id not in active_sessions:
        logger.warning(f"Received frame from unknown session: {session_id}")  # Keep warnings
        emit('session_error', {'message': 'Invalid session'})
        return
    
    active_sessions[session_id]['last_activity'] = time.time()
    
    try:
        image_data = data['image'].split(',')[1]
        image_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(image_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        detector = active_sessions[session_id]['detector']
        result = detector.process_frame(frame)
        
        display_frame = result['display_frame']
        debug_frame = result['debug_frame']
        
        # Encode frames
        _, buffer_disp = cv2.imencode('.jpg', display_frame)
        disp_b64 = base64.b64encode(buffer_disp).decode('utf-8')
        
        # Always encode debug frame if it exists
        debug_b64 = None
        if debug_frame is not None:
            _, buffer_dbg = cv2.imencode('.jpg', debug_frame)
            debug_b64 = base64.b64encode(buffer_dbg).decode('utf-8')
        
        # Prepare data to emit
        emit_data = {
            'image': f"data:image/jpeg;base64,{disp_b64}",
            'debug_image': f"data:image/jpeg;base64,{debug_b64}" if debug_b64 else None,
            'challenge': result['challenge_text'],
            'action_completed': result['action_completed'],
            'word_completed': result['word_completed'],
            'time_remaining': result['time_remaining'],
            'verification_result': result['verification_result'],
            'exit_flag': result['exit_flag']
        }
        
        if config.DEBUG:  # Only log if debug is enabled
            logger.debug(f"Emitting processed frame: challenge={result['challenge_text']}, " 
                        f"action_completed={result['action_completed']}, "
                        f"has_debug_frame={debug_b64 is not None}")
        
        emit('processed_frame', emit_data)
        
        if result['exit_flag']:
            active_sessions[session_id]['attempts'] = active_sessions[session_id].get('attempts', 0) + 1
            
            if result['verification_result'] == 'PASS' or active_sessions[session_id]['attempts'] >= 3:
                if code and code in verification_codes:
                    verification_codes[code]['status'] = 'completed'
                    verification_codes[code]['result'] = result['verification_result']
                    requester_id = verification_codes[code]['requester_id']
                    emit('verification_result', {
                        'result': result['verification_result'],
                        'code': code
                    }, room=requester_id)
        
    except Exception as e:
        logger.error(f"Error processing frame: {e}")  # Keep error logging always on
        emit('error', {'message': str(e)})

@socketio.on('verification_complete')
def handle_verification_complete(data):
    """Handle verification completion notification."""
    code = data.get('code')
    result = data.get('result')
    
    if code and code in verification_codes:
        verification_codes[code]['status'] = 'completed'
        verification_codes[code]['result'] = result
        
        requester_id = verification_codes[code]['requester_id']
        emit('verification_result', {
            'result': result,
            'code': code
        }, room=requester_id)
        
        logger.info(f"Verification {code} completed with result: {result}")

if __name__ == '__main__':
    cleanup_thread = threading.Thread(target=cleanup_inactive_sessions)
    cleanup_thread.daemon = True
    cleanup_thread.start()
    
    port = int(os.environ.get('PORT', 8080))
    
    socketio.run(
        app,
        host='0.0.0.0',
        port=port,
        debug=config.DEBUG,
        certfile='cert.pem',
        keyfile='key.pem'
    )


===== action_detector.py =====

"""Action detection module for liveness verification."""

import cv2
import numpy as np
import logging
import dlib
from typing import List, Tuple, Dict, Any, Optional
from collections import deque

class ActionDetector:
    """Detects specific actions for liveness verification."""
    
    def __init__(self, config):
        """Initialize the action detector with configuration."""
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Load dlib face detector and shape predictor
        self.dlib_detector = dlib.get_frontal_face_detector()
        try:
            self.dlib_predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
            self.using_dlib = True
            self.logger.info("Using dlib for facial landmark detection in ActionDetector")
        except Exception as e:
            self.logger.error(f"Could not load dlib shape predictor: {e}")
            self.using_dlib = False
            raise ValueError("Dlib shape predictor is required for action detection")
        
        # Action detection variables
        self.current_action = None
        self.action_completed = False
        self.action_start_time = None
        
        # Head pose tracking
        self.face_positions = deque(maxlen=config.FACE_POSITION_HISTORY_LENGTH)
        self.face_angles = deque(maxlen=config.FACE_POSITION_HISTORY_LENGTH)
        self.head_pose = "center"  # center, left, right, up, down
    
    def set_action(self, action: str) -> None:
        """
        Set the current action to detect.
        
        Args:
            action: Action name ("left", "right", "up", "down")
        """
        self.current_action = action
        self.action_completed = False
        self.action_start_time = None
        self.logger.debug(f"Action set to: {action}")
    
    def detect_head_pose(self, frame: np.ndarray, face_rect: Tuple[int, int, int, int]) -> str:
        """
        Detect head pose (left, right, up, down, center).
        
        Args:
            frame: Input video frame
            face_rect: Face rectangle (x, y, w, h)
            
        Returns:
            Head pose as string: "left", "right", "up", "down", or "center"
        """
        if face_rect is None:
            return self.head_pose
            
        x, y, w, h = face_rect
        
        face_center_x = x + w/2
        frame_center_x = frame.shape[1] / 2
        face_center_y = y + h/2
        frame_center_y = frame.shape[0] / 2
        
        x_offset = face_center_x - frame_center_x
        y_offset = face_center_y - frame_center_y
        
        x_offset_normalized = x_offset / (frame.shape[1] / 2)
        y_offset_normalized = y_offset / (frame.shape[0] / 2)
        
        self.face_angles.append((x_offset_normalized, y_offset_normalized))
        
        if len(self.face_angles) >= 5:
            angles_list = list(self.face_angles)
            avg_x_offset = sum(a[0] for a in angles_list) / len(angles_list)
            avg_y_offset = sum(a[1] for a in angles_list) / len(angles_list)
            
            x_threshold = self.config.HEAD_POSE_THRESHOLD_X
            y_threshold_up = self.config.HEAD_POSE_THRESHOLD_Y_UP
            y_threshold_down = self.config.HEAD_POSE_THRESHOLD_Y_DOWN
            
            self.logger.debug(f"Head position - X offset: {avg_x_offset:.2f}, Y offset: {avg_y_offset:.2f}")
            
            old_pose = self.head_pose
            if avg_x_offset < -x_threshold:
                self.head_pose = "right"
                if old_pose != "right":
                    self.logger.debug("RIGHT detected!")
            elif avg_x_offset > x_threshold:
                self.head_pose = "left"
                if old_pose != "left":
                    self.logger.debug("LEFT detected!")
            elif avg_y_offset < -y_threshold_up:
                self.head_pose = "up"
                if old_pose != "up":
                    self.logger.debug("UP detected!")
            elif avg_y_offset > y_threshold_down:
                self.head_pose = "down"
                if old_pose != "down":
                    self.logger.debug("DOWN detected!")
            else:
                self.head_pose = "center"
                if old_pose != "center":
                    self.logger.debug("CENTER detected!")
            
            # Draw direction indicator for debugging
            center_x = int(frame.shape[1] / 2)
            center_y = int(frame.shape[0] / 2)
            direction_x = int(center_x + avg_x_offset * 100)
            direction_y = int(center_y + avg_y_offset * 100)
            cv2.line(frame, (center_x, center_y), (direction_x, direction_y), (0, 255, 255), 2)
        
        return self.head_pose
    
    def detect_action(self, frame: np.ndarray, face_rect: Tuple[int, int, int, int]) -> bool:
        """
        Detect the specified action.
        
        Args:
            frame: Input video frame
            face_rect: Face rectangle (x, y, w, h)
            
        Returns:
            True if action is detected, False otherwise
        """
        if self.current_action is None or face_rect is None:
            return False
            
        current_pose = self.detect_head_pose(frame, face_rect)
        self.action_completed = current_pose.lower() == self.current_action.lower()
        
        return self.action_completed
    
    def is_action_completed(self):
        """Check if the current action is completed."""
        return self.action_completed 


===== static/css/style.css =====

* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #e0e0e0;
    background-color: #121212;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1 {
    text-align: center;
    margin-bottom: 30px;
    color: #ffffff;
    font-weight: 300;
    letter-spacing: 1px;
}

h2 {
    color: #ffffff;
    font-weight: 400;
    margin-bottom: 15px;
    letter-spacing: 0.5px;
}

h3 {
    color: #ffffff;
    font-weight: 400;
    margin-bottom: 10px;
    letter-spacing: 0.5px;
}

/* Card layout for landing page */
.card-container {
    display: flex;
    justify-content: center;
    gap: 30px;
    margin-bottom: 40px;
}

.card {
    background-color: #1e1e1e;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    width: 100%;
    max-width: 450px;
    border: 1px solid #333;
    transition: transform 0.3s, box-shadow 0.3s;
}

.card:hover {
    transform: translateY(-5px);
    box-shadow: 0 15px 30px rgba(0, 0, 0, 0.4);
}

.card p {
    color: #bbb;
    margin-bottom: 20px;
}

.input-group {
    display: flex;
    margin-top: 20px;
}

input[type="text"] {
    flex: 1;
    padding: 12px 15px;
    border: 1px solid #333;
    background-color: #252525;
    color: #fff;
    border-radius: 30px 0 0 30px;
    font-size: 16px;
    outline: none;
    transition: border-color 0.3s;
}

input[type="text"]:focus {
    border-color: #3498db;
}

.input-group .button {
    border-radius: 0 30px 30px 0;
}

.verification-code {
    font-size: 36px;
    font-weight: bold;
    letter-spacing: 5px;
    color: #3498db;
    background-color: #252525;
    padding: 15px;
    border-radius: 8px;
    margin: 15px 0;
    text-align: center;
}

.status-waiting {
    color: #f39c12;
    font-weight: 500;
    margin-top: 20px;
    text-align: center;
}

.status-success {
    color: #4cd137;
    font-weight: 500;
    margin-top: 20px;
    text-align: center;
}

.status-failed {
    color: #e84118;
    font-weight: 500;
    margin-top: 20px;
    text-align: center;
}

.session-info {
    background-color: #1e1e1e;
    padding: 10px 20px;
    border-radius: 30px;
    display: inline-block;
    margin: 0 auto 20px;
    text-align: center;
    border: 1px solid #333;
}

.session-info p {
    margin: 0;
}

#session-code {
    font-weight: bold;
    color: #3498db;
    letter-spacing: 1px;
}

.footer {
    text-align: center;
    margin-top: 40px;
    color: #666;
    font-size: 14px;
}

/* Video container styles */
.video-container {
    display: flex;
    justify-content: space-between;
    margin-bottom: 20px;
    gap: 20px;
}

.video-container.single-video {
    justify-content: center;
}

.video-wrapper {
    position: relative;
    width: 48%;
    max-width: 640px;
    border-radius: 12px;
    overflow: hidden;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
}

.single-video .video-wrapper {
    width: 100%;
    max-width: 640px;
}

#webcam, #overlay {
    width: 100%;
    height: auto;
    border-radius: 12px;
}

#overlay {
    position: absolute;
    top: 0;
    left: 0;
}

#processed-frame-container {
    width: 48%;
}

#processed-frame {
    width: 100%;
    height: auto;
    border-radius: 12px;
    border: 1px solid #333;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
}

.challenge-container {
    background-color: #1e1e1e;
    padding: 20px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    margin-bottom: 20px;
    border: 1px solid #333;
}

.challenge-text {
    font-size: 24px;
    font-weight: 500;
    text-align: center;
    margin-bottom: 15px;
    color: #ffffff;
    letter-spacing: 0.5px;
}

.status-container {
    display: flex;
    justify-content: space-around;
    background-color: #252525;
    padding: 15px;
    border-radius: 8px;
}

.status-item {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-size: 18px;
}

.status-item span:first-child {
    color: #999;
    margin-bottom: 5px;
    font-size: 14px;
    text-transform: uppercase;
    letter-spacing: 1px;
}

.result-container {
    background-color: #1e1e1e;
    padding: 25px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    margin-bottom: 20px;
    text-align: center;
    border: 1px solid #333;
}

.result-text {
    font-size: 1.5rem;
    font-weight: bold;
    text-align: center;
    padding: 1rem;
    border-radius: 5px;
    margin-bottom: 1rem;
}

.result-text.success {
    background-color: rgba(46, 204, 113, 0.3);
    color: #27ae60;
}

.result-text.failure {
    background-color: rgba(231, 76, 60, 0.3);
    color: #c0392b;
}

.button {
    background-color: #2980b9;
    color: white;
    border: none;
    padding: 12px 25px;
    font-size: 16px;
    border-radius: 30px;
    cursor: pointer;
    transition: all 0.3s;
    text-transform: uppercase;
    letter-spacing: 1px;
    font-weight: 500;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

.button:hover {
    background-color: #3498db;
    transform: translateY(-2px);
    box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15);
}

.button:active {
    transform: translateY(0);
}

.instructions {
    background-color: #1e1e1e;
    padding: 20px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
    border: 1px solid #333;
}

.instructions h2 {
    margin-bottom: 15px;
    color: #ffffff;
    font-weight: 400;
    letter-spacing: 0.5px;
}

.instructions p {
    color: #bbb;
    margin-bottom: 10px;
    padding-left: 15px;
    position: relative;
}

.instructions p:before {
    content: "•";
    position: absolute;
    left: 0;
    color: #3498db;
}

.hidden {
    display: none;
}

@media (max-width: 768px) {
    .card-container {
        flex-direction: column;
        align-items: center;
    }
    
    .video-container {
        flex-direction: column;
    }
    
    .video-wrapper, #processed-frame-container {
        width: 100%;
    }
    
    .challenge-text {
        font-size: 20px;
    }
    
    .status-item {
        font-size: 16px;
    }
}

.error-container {
    background-color: #1e1e1e;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    text-align: center;
    border: 1px solid #333;
    margin: 50px auto;
    max-width: 500px;
}

.error-icon {
    font-size: 48px;
    color: #e74c3c;
    margin-bottom: 20px;
}

.error-message {
    font-size: 24px;
    color: #ffffff;
    margin-bottom: 30px;
}

.verify-status {
    margin-top: 15px;
    padding: 10px;
    border-radius: 8px;
    text-align: center;
    font-weight: 500;
    transition: all 0.3s ease;
    height: 40px;
    display: flex;
    align-items: center;
    justify-content: center;
}

.verify-status.error {
    background-color: rgba(231, 76, 60, 0.2);
    color: #e74c3c;
    border: 1px solid rgba(231, 76, 60, 0.3);
}

.verify-status.success {
    background-color: rgba(46, 204, 113, 0.2);
    color: #2ecc71;
    border: 1px solid rgba(46, 204, 113, 0.3);
}

.verify-status.info {
    background-color: rgba(52, 152, 219, 0.2);
    color: #3498db;
    border: 1px solid rgba(52, 152, 219, 0.3);
}

/* Success/Failure overlay effects */
.success-overlay {
    box-shadow: 0 0 20px 10px rgba(46, 204, 113, 0.6);
    filter: sepia(0.2) saturate(1.5) brightness(1.1) hue-rotate(60deg);
    border: 3px solid #2ecc71;
}

.failure-overlay {
    box-shadow: 0 0 20px 10px rgba(231, 76, 60, 0.6);
    filter: sepia(0.3) saturate(1.5) brightness(0.9) hue-rotate(-20deg);
    border: 3px solid #e74c3c;
}

/* Update the debug frame styles */
#debug-frame {
    width: 100%;
    height: auto;
    border-radius: 12px;
    border: 1px solid #333;
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
    object-fit: contain; /* Keep aspect ratio */
}

#processed-frame-container, .video-wrapper {
    width: 48%;
    max-width: 640px;
    height: auto;
} 


===== templates/error.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error - Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Error</h1>
        
        <div class="error-container">
            <div class="error-icon">❌</div>
            <div class="error-message">{{ message }}</div>
            <a href="{{ redirect_url }}" class="button">Return to Home</a>
        </div>
    </div>
    
    <script>
        // Automatically redirect after 5 seconds
        setTimeout(() => {
            window.location.href = "{{ redirect_url }}";
        }, 5000);
    </script>
</body>
</html> 


===== templates/index.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Liveness Verification System</h1>
        
        <div class="card-container">
            <div class="card">
                <h2>Request Verification</h2>
                <p>Generate a code and send it to someone to verify they're human</p>
                <div id="request-container">
                    <button id="generate-code-btn" class="button">Generate Code</button>
                    <div id="code-display" class="hidden">
                        <h3>Your verification code:</h3>
                        <div class="verification-code"><span id="verification-code"></span></div>
                        <p>Share this code with the person you want to verify</p>
                        <div id="verification-status" class="status-waiting">
                            Waiting for verification...
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2>Complete Verification</h2>
                <p>Enter a verification code to prove you're human</p>
                <div id="verify-container">
                    <div class="input-group">
                        <input type="text" id="code-input" placeholder="Enter 6-digit code" maxlength="6" pattern="[0-9]{6}">
                        <button id="submit-code-btn" class="button">Verify</button>
                    </div>
                    <!-- verify-status will be added here by JavaScript -->
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>Secure liveness detection for remote identity verification</p>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <script src="{{ url_for('static', filename='js/landing.js') }}"></script>
</body>
</html> 


===== templates/verify.html =====

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liveness Verification</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Liveness Verification</h1>
        <div class="session-info">
            <p>Session Code: <span id="session-code">{{ session_code }}</span></p>
        </div>
        
        <div class="video-container">
            <div class="video-wrapper">
                <video id="webcam" autoplay playsinline></video>
                <canvas id="overlay" style="display: none;"></canvas>
            </div>
            <div id="processed-frame-container">
                <img id="debug-frame" alt="Debug Frame" class="hidden">
            </div>
        </div>
        
        <div class="challenge-container">
            <div id="challenge-text" class="challenge-text">Waiting for challenge...</div>
            <div class="status-container">
                <div class="status-item">
                    <span>Action</span>
                    <span id="action-status">❌</span>
                </div>
                <div class="status-item">
                    <span>Word</span>
                    <span id="word-status">❌</span>
                </div>
                <div class="status-item">
                    <span>Time</span>
                    <span id="time-remaining">0s</span>
                </div>
            </div>
        </div>
        
        <div id="result-container" class="result-container hidden">
            <div id="result-text" class="result-text"></div>
            <button id="reset-button" class="button">Try Again</button>
        </div>
        
        <div class="instructions">
            <h2>Instructions</h2>
            <p>Allow camera access when prompted</p>
            <p>Follow the challenge instructions displayed on screen</p>
            <p>Complete both the action and speech parts of the challenge</p>
            <p>You have up to 3 attempts to verify your identity</p>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <script src="{{ url_for('static', filename='js/app.js') }}"></script>
</body>
</html>


===== static/js/app.js =====

document.addEventListener('DOMContentLoaded', () => {
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const debugFrame = document.getElementById('debug-frame');
    const challengeText = document.getElementById('challenge-text');
    const actionStatus = document.getElementById('action-status');
    const wordStatus = document.getElementById('word-status');
    const timeRemaining = document.getElementById('time-remaining');
    const resultContainer = document.getElementById('result-container');
    const resultText = document.getElementById('result-text');
    const resetButton = document.getElementById('reset-button');
    const videoContainer = document.querySelector('.video-container');
    const sessionCode = document.getElementById('session-code').textContent;
    
    let socket;
    let isProcessing = false;
    let stream = null;
    let verificationAttempts = 0;
    const MAX_VERIFICATION_ATTEMPTS = 3;
    let isDebugMode = true; // Set to true to show debug output
    let showDebugFrame = true; // Set to true to show debug frame
    let frameCount = 0;
    
    // Initialize Socket.IO connection
    function initSocket() {
        console.log('Initializing socket connection for verification');
        socket = io();
        
        socket.on('connect', () => {
            if (isDebugMode) console.log('Connected to server for verification');
            socket.emit('join_verification', { code: sessionCode });
            socket.emit('get_debug_status');
            if (isDebugMode) console.log('Sent join_verification and get_debug_status events');
        });
        
        socket.on('debug_status', (data) => {
            isDebugMode = data.debug;
            showDebugFrame = data.showDebugFrame;
            if (isDebugMode) {
                console.log(`Debug mode: ${isDebugMode}, Show debug frame: ${showDebugFrame}`);
            }
            
            isProcessing = true;  // Start processing frames after receiving debug status
            
            // Show/hide debug frame based on settings
            debugFrame.style.display = showDebugFrame ? 'block' : 'none';
        });
        
        socket.on('session_joined', (data) => {
            console.log('Verification session joined:', data);
            isProcessing = true;
        });
        
        socket.on('processed_frame', (data) => {
            if (isDebugMode && frameCount % 30 === 0) {
                console.log('Received processed frame:', {
                    hasImage: !!data.image,
                    hasDebugImage: !!data.debug_image,
                    action_completed: data.action_completed,
                    word_completed: data.word_completed,
                    time_remaining: data.time_remaining
                });
            }
            
            // Send all data to the debug frame instead
            if (data.image && debugFrame) {
                debugFrame.src = data.debug_image || data.image;
                debugFrame.style.display = showDebugFrame ? 'block' : 'none';
            }
            
            // Update challenge text and status
            if (data.challenge) {
                challengeText.textContent = data.challenge;
                challengeText.classList.remove('hidden');
            } else {
                challengeText.textContent = 'Waiting for challenge...';
            }
            
            // Update action status
            actionStatus.textContent = data.action_completed ? '✅' : '❌';
            actionStatus.className = data.action_completed ? 'status-complete' : 'status-incomplete';
            
            // Update word status
            wordStatus.textContent = data.word_completed ? '✅' : '❌';
            wordStatus.className = data.word_completed ? 'status-complete' : 'status-incomplete';
            
            // Update time remaining
            if (data.time_remaining !== undefined) {
                timeRemaining.textContent = Math.max(0, Math.ceil(data.time_remaining)) + 's';
            }
            
            // Handle verification result
            if (data.verification_result !== 'PENDING') {
                isProcessing = false;
                
                if (data.verification_result === 'PASS') {
                    resultText.textContent = 'Verification Successful!';
                    resultText.className = 'result-text success';
                    applyVideoEffect('success');
                    
                    // Redirect to main page after 3 seconds
                    setTimeout(() => {
                        window.location.href = '/';
                    }, 3000);
                } else {
                    resultText.textContent = 'Verification Failed!';
                    resultText.className = 'result-text failure';
                    applyVideoEffect('failure');
                    
                    verificationAttempts++;
                    if (verificationAttempts >= MAX_VERIFICATION_ATTEMPTS) {
                        resultText.textContent = 'Maximum attempts reached. Verification failed.';
                        setTimeout(() => {
                            window.location.href = '/';
                        }, 3000);
                    } else {
                        // Allow another attempt after 3 seconds
                        setTimeout(() => {
                            resultContainer.classList.add('hidden');
                            isProcessing = true;
                            removeVideoEffect();
                        }, 3000);
                    }
                }
                
                resultContainer.classList.remove('hidden');
            }
            
            // Continue processing frames if not paused
            if (isProcessing) {
                requestAnimationFrame(captureAndSendFrame);
            }
            if (isDebugMode) {
                console.log('Debug frame status:', {
                    hasDebugImage: !!data.debug_image,
                    debugFrameElement: !!debugFrame,
                    debugFrameClass: debugFrame.className,
                    showDebugFrame: showDebugFrame
                });
            }
            
            if (isDebugMode) {
                console.log('Processed frame response:', {
                    hasImage: !!data.image,
                    hasDebugImage: !!data.debug_image,
                    challenge: data.challenge
                });
                
                // Let's examine the actual URLs being set
                if (data.image) {
                    console.log('Image URL preview:', data.image.substring(0, 50) + '...');
                }
                if (data.debug_image) {
                    console.log('Debug image URL preview:', data.debug_image.substring(0, 50) + '...');
                }
            }
            
            // Add this to the processed_frame handler, right after the time remaining update
            if (data.time_remaining <= 0 && data.verification_result === 'PENDING') {
                // Time expired but no conclusive result yet - force reset
                if (isDebugMode) console.log('Challenge timed out, resetting');
                socket.emit('reset', { code: sessionCode });
                verificationAttempts++;
                
                if (verificationAttempts >= MAX_VERIFICATION_ATTEMPTS) {
                    resultText.textContent = 'Maximum attempts reached. Verification failed.';
                    resultText.className = 'result-text failure';
                    resultContainer.classList.remove('hidden');
                    isProcessing = false;
                } else {
                    // Tell user what happened
                    challengeText.textContent = `Attempt ${verificationAttempts+1} of ${MAX_VERIFICATION_ATTEMPTS}...`;
                    setTimeout(() => {
                        challengeText.textContent = 'Waiting for new challenge...';
                    }, 2000);
                }
            }
        });
        
        socket.on('error', (data) => {
            console.error('Server error:', data.message);
            alert('Server error: ' + data.message);
        });
        
        socket.on('max_attempts_reached', () => {
            isProcessing = false;
            resultText.textContent = 'Maximum verification attempts reached.';
            resultContainer.classList.remove('hidden');
            applyFailureEffect();
        });
        
        socket.on('disconnect', () => {
            console.log('Disconnected from server');
            isProcessing = false;
        });
        
        socket.on('join_success', (data) => {
            if (isDebugMode) {
                console.log('Successfully joined verification session');
            }
            isProcessing = true;  // Set processing to true after successful join
        });
        
        socket.on('challenge', (data) => {
            if (isDebugMode) {
                console.log('Received challenge:', data);
            }
            
            // Update the challenge text in the UI
            if (data && data.text) {
                challengeText.textContent = data.text;
            }
        });
        
        socket.onAny((event, ...args) => {
            if (isDebugMode) {
                console.log(`Received socket event: ${event}`, args);
            }
            
            // Force processing to start when any event is received after connection
            if (event !== 'connect' && !isProcessing) {
                console.log('Starting frame processing');
                isProcessing = true;
                
                // Add this to ensure the frame capture interval is active
                if (isDebugMode) console.log('Making sure frame capture interval is active');
                
                // This should restart the frame interval if it's not already running
                startFrameCapture();
            }
        });
        
        // Add this event monitoring for processed_frame - don't modify existing code
        socket.onAny((event) => {
            // Only monitor for the processed_frame event to avoid console flooding
            if (event === 'processed_frame' && isDebugMode) {
                console.log('Detected processed_frame event!');
            }
        });
        
        // Add this at the end of your initialize function or after socket = io()
        socket.on('connect', () => {
            if (isDebugMode) {
                console.log('Socket connection state:', {
                    id: socket.id,
                    connected: socket.connected,
                    disconnected: socket.disconnected
                });
            }
        });
    }
    
    // Apply success effect to video
    function applySuccessEffect() {
        videoContainer.classList.add('success-overlay');
        resultText.textContent = 'Verification successful!';
        resultText.className = 'result-text success';
        resultContainer.classList.remove('hidden');
    }
    
    // Apply failure effect to video
    function applyFailureEffect() {
        videoContainer.classList.add('failure-overlay');
        resultText.textContent = 'Verification failed. Please try again.';
        resultText.className = 'result-text failure';
        resultContainer.classList.remove('hidden');
    }
    
    // Remove video effects
    function removeVideoEffect() {
        videoContainer.classList.remove('success-overlay', 'failure-overlay');
        resultContainer.classList.add('hidden');
    }
    
    // Handle verification result
    function handleVerificationResult(result) {
        if (result === 'PASS') {
            resultText.textContent = 'Verification successful!';
            resultText.className = 'result-text success';
        } else if (result === 'FAIL') {
            resultText.textContent = 'Verification failed.';
            resultText.className = 'result-text failure';
        }
    }
    
    // Capture and send frame to server
    function captureAndSendFrame() {
        if (!isProcessing) {
            if (isDebugMode) console.log('Not processing frames - isProcessing is false');
            return;
        }
        
        if (isDebugMode) console.log('Processing frames - capturing and sending frame');
        
        try {
            // Draw video frame to canvas
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            
            // Get image data from canvas
            const imageData = canvas.toDataURL('image/jpeg', 0.8);
            
            if (isDebugMode) console.log(`Sending frame ${frameCount} to server`);
            
            // Send to server
            socket.emit('process_frame', {
                image: imageData,
                code: sessionCode
            });
            
            frameCount++;
            if (isDebugMode && frameCount % 30 === 0) {
                console.log(`Sent ${frameCount} frames`);
            }
        } catch (err) {
            console.error('Error capturing frame:', err);
        }
    }
    
    // Initialize webcam
    async function initWebcam() {
        try {
            stream = await navigator.mediaDevices.getUserMedia({
                video: {
                    width: { ideal: 640 },
                    height: { ideal: 480 },
                    facingMode: 'user'
                },
                audio: true
            });
            video.srcObject = stream;
            console.log('Webcam access granted, waiting for video to load');
            
            video.onloadedmetadata = () => {
                console.log('Video loaded, starting frame capture');
                video.play().catch(err => console.error('Error playing video:', err));
                captureAndSendFrame();
            };
        } catch (err) {
            console.error('Error accessing webcam:', err);
            alert('Error accessing webcam: ' + err.message);
        }
    }
    
    // Handle reset button click
    resetButton.addEventListener('click', () => {
        if (socket && socket.connected) {
            console.log('Sending reset request');
            socket.emit('reset', { code: sessionCode });
            resultContainer.classList.add('hidden');
            isProcessing = true;
            removeVideoEffect();
        }
    });
    
    // Initialize everything
    function init() {
        console.log('Initializing verification page');
        initSocket();
        initWebcam();
    }
    
    // Cleanup on page unload
    window.addEventListener('beforeunload', () => {
        if (socket && socket.connected) {
            socket.disconnect();
        }
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
        }
    });
    
    // Add this helper function that should already exist in your code
    function startFrameCapture() {
        if (isDebugMode) console.log('Starting frame capture interval');
        // Call captureAndSendFrame immediately once
        captureAndSendFrame();
        
        // Then set up interval - use a reasonably fast interval (100ms = 10fps)
        setInterval(captureAndSendFrame, 100);
    }
    
    init();
});


===== static/js/landing.js =====

document.addEventListener('DOMContentLoaded', () => {
    const generateCodeBtn = document.getElementById('generate-code-btn');
    const codeDisplay = document.getElementById('code-display');
    const verificationCode = document.getElementById('verification-code');
    const verificationStatus = document.getElementById('verification-status');
    const codeInput = document.getElementById('code-input');
    const submitCodeBtn = document.getElementById('submit-code-btn');
    const verifyContainer = document.getElementById('verify-container');
    const verifyStatus = document.createElement('div');
    
    verifyStatus.className = 'verify-status';
    verifyContainer.appendChild(verifyStatus);
    
    let socket;
    let currentCode = null;
    
    // Initialize Socket.IO connection
    function initSocket() {
        console.log('Initializing socket connection');
        socket = io();
        
        socket.on('connect', () => {
            console.log('Connected to server');
        });
        
        socket.on('disconnect', () => {
            console.log('Disconnected from server');
        });
        
        socket.on('error', (data) => {
            console.error('Socket error:', data);
        });
        
        socket.on('verification_code', (data) => {
            console.log('Received verification code:', data.code);
            currentCode = data.code;
            verificationCode.textContent = currentCode;
            codeDisplay.classList.remove('hidden');
            generateCodeBtn.classList.add('hidden');
            
            // Update status
            verificationStatus.textContent = 'Waiting for verification...';
            verificationStatus.className = 'status-waiting';
        });
        
        socket.on('verification_started', (data) => {
            verificationStatus.textContent = 'Verification in progress...';
            verificationStatus.className = 'status-waiting';
        });
        
        socket.on('verification_result', (data) => {
            if (data.result === 'PASS') {
                verificationStatus.textContent = 'Verification PASSED ✅';
                verificationStatus.className = 'status-success';
            } else {
                verificationStatus.textContent = 'Verification FAILED ❌';
                verificationStatus.className = 'status-failed';
            }
            
            // Re-enable generate button after 5 seconds
            setTimeout(() => {
                generateCodeBtn.classList.remove('hidden');
                codeDisplay.classList.add('hidden');
                currentCode = null;
            }, 5000);
        });
        
        socket.on('code_error', (data) => {
            showVerifyError(data.message);
        });
    }
    
    // Generate verification code
    generateCodeBtn.addEventListener('click', () => {
        console.log('Generate code button clicked');
        if (socket && socket.connected) {
            socket.emit('generate_code');
        } else {
            console.error('Socket not connected');
            // Try to reconnect
            initSocket();
            setTimeout(() => {
                if (socket && socket.connected) {
                    socket.emit('generate_code');
                } else {
                    console.error('Failed to reconnect socket');
                }
            }, 1000);
        }
    });
    
    // Show verification error
    function showVerifyError(message) {
        verifyStatus.textContent = message;
        verifyStatus.className = 'verify-status error';
        
        // Clear after 3 seconds
        setTimeout(() => {
            verifyStatus.textContent = '';
            verifyStatus.className = 'verify-status';
        }, 3000);
    }
    
    // Show verification success
    function showVerifySuccess(message) {
        verifyStatus.textContent = message;
        verifyStatus.className = 'verify-status success';
        
        // Clear after 1 second before redirect
        setTimeout(() => {
            verifyStatus.textContent = '';
            verifyStatus.className = 'verify-status';
        }, 1000);
    }
    
    // Submit verification code
    submitCodeBtn.addEventListener('click', () => {
        const code = codeInput.value.trim();
        
        if (code.length !== 6 || !/^\d+$/.test(code)) {
            showVerifyError('Please enter a valid 6-digit code');
            return;
        }
        
        // Show loading state
        submitCodeBtn.disabled = true;
        submitCodeBtn.textContent = 'Checking...';
        verifyStatus.textContent = 'Validating code...';
        verifyStatus.className = 'verify-status info';
        
        // Check if code exists via fetch API
        fetch(`/check_code/${code}`)
            .then(response => {
                if (!response.ok) {
                    throw new Error(`Server error: ${response.status}`);
                }
                return response.json();
            })
            .then(data => {
                submitCodeBtn.disabled = false;
                submitCodeBtn.textContent = 'Verify';
                
                if (data.valid) {
                    showVerifySuccess('Code valid! Redirecting...');
                    // Redirect to verification page with code
                    setTimeout(() => {
                        window.location.href = `/verify/${code}`;
                    }, 1000);
                } else {
                    showVerifyError('Invalid code. Please check and try again.');
                }
            })
            .catch(error => {
                console.error('Error checking code:', error);
                submitCodeBtn.disabled = false;
                submitCodeBtn.textContent = 'Verify';
                showVerifyError('Error checking code. Please try again.');
            });
    });
    
    // Handle Enter key in code input
    codeInput.addEventListener('keypress', (e) => {
        if (e.key === 'Enter') {
            submitCodeBtn.click();
        }
    });
    
    // Initialize
    initSocket();
}); 

